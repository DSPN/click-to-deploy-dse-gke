---
# Source: dse-server/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dse-dse-server-opsc-admin-pwd-secret
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server-opsc
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
type: Opaque
data:
  password: "ZGF0YXN0YXgxIQ=="

---
# Source: dse-server/templates/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dse-dse-server-opsc-agent-configmap
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
data:

  
  address.yaml: |
    # stomp_interface: 127.0.0.1
    # use_ssl: 0
    
  
  datastax-agent-env.sh: |
    JVM_OPTS="$JVM_OPTS -Xmx1024M"
    
  
  log4j.properties: |
    # Based on the example properties given at http://logging.apache.org/log4j/1.2/manual.html
    # Set root logger level to DEBUG and its only appender to A1.
    log4j.rootLogger=INFO,R,stdout
    
    log4j.logger.org.apache.http=OFF
    log4j.logger.org.eclipse.jetty=WARN,stdout
    log4j.logger.com.datastax.driver=WARN,R
    log4j.additivity.com.datastax.driver=false
    # Silence "missing LZ4" warning
    log4j.logger.com.datastax.driver.core.FrameCompressor=ERROR,R
    
    # stdout
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=opsagent.AlternatingEnhancedPatternLayout
    log4j.appender.stdout.layout.MainPattern=%5p [%t] %d{ISO8601} %m%n %throwable{200}
    log4j.appender.stdout.layout.AlternatePattern=%5p [%t] %d{ISO8601} %m%n %throwable{3}
    log4j.appender.stdout.layout.ToMatch=com.datastax.driver
    
    # rolling log file
    log4j.appender.R=org.apache.log4j.RollingFileAppender
    log4j.appender.R.maxFileSize=20MB
    log4j.appender.R.maxBackupIndex=5
    log4j.appender.R.layout=opsagent.AlternatingEnhancedPatternLayout
    log4j.appender.R.layout.MainPattern=%5p [%t] %d{ISO8601} %m%n %throwable{200}
    log4j.appender.R.layout.AlternatePattern=%5p [%t] %d{ISO8601} %m%n %throwable{3}
    log4j.appender.R.layout.ToMatch=com.datastax.driver
    log4j.appender.R.File=/var/log/datastax-agent/agent.log
    
  
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dse-dse-server-opsc-event-plugins-configmap
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
data:

  
  email.conf: |
    [email]
    # set to 1 to enable email
    enabled=0
    
    # levels can be comma delimited list of any of the following:
    # DEBUG,INFO,WARN,ERROR,CRITICAL,ALERT
    # If left empty, will listen for all levels
    levels=
    
    # clusters is a comma delimited list of cluster names for which
    # this alert config will be eligible to run.
    # If left empty, this alert will will be called for events on all clusters
    clusters=
    
    smtp_host=localhost
    smtp_port=25
    smtp_user=
    smtp_pass=
    smtp_use_ssl=0
    smtp_use_tls=0
    smtp_retries=1
    smtp_timeout=5
    
    to_addr=email@domain.com
    from_addr=email@domain.com
    
    # Customizable templates for subject and body. The key specified in {}'s must map to the items provided in json map at the end of
    # the emails. For example, some available keys are:
    #     node, cluster, datetime, level_str, message, target_node, event_source, success, api_source_ip, user, source_node
    # More advanced formatting options explained here: https://docs.python.org/2/library/string.html#formatspec
    
    subject=[{level_str}] OpsCenter Event on {cluster} - {message}
    
    template=Message: {message}
      Time:    {datetime}
      Level:   {level_str}
      Cluster: {cluster}
      Node:    {node}
      User:    {user}
    
      **********
      {parsable}
    
  
  hipchat_posturl.conf: |
    [posturl]
    enabled=0
    
    # levels can be comma-delimited list of any of the following:
    # DEBUG,INFO,WARN,ERROR,CRITICAL,ALERT
    # If left empty, all levels will be posted
    levels=
    
    # clusters is a comma-delimited list of cluster names for which
    # this alert config will be eligible to run.
    # If left empty, this alert will be called for events on all clusters
    clusters=
    
    # You can learn more about how to get authentication tokens at
    # https://www.hipchat.com/docs/apiv2/auth
    auth_token=
    
    # You can find a list of room IDs for your organization at
    # https://api.hipchat.com/v2/room?max-results=1000&auth_token=<your auth token>
    room_id=
    
    # You may use tokens found within the default event data for or in
    # values. For example, some available keys are:
    #   cluster, time, level_str, message, target_node, event_source, success, api_source_ip, user, source_node
    # Keys must be encapsulated in {brackets}.
    message=<b>{level_str} from {cluster}</b>: {message}
    
    # Valid values: yellow, green, red, purple, gray, random.
    color=yellow
    
    # Determines how the message is treated by HipChat applications
    #   html:
    #     Message is rendered as HTML and receives no special treatment.
    #     Must be valid HTML and entities must be escaped (e.g.: '&amp;' instead of '&').
    #     May contain basic tags: a, b, i, strong, em, br, img, pre, code, lists, tables.
    #   text:
    #     Message is treated just like a message sent by a user.
    #     Can include @mentions, emoticons, pastes, and auto-detected URLs (Twitter, YouTube, images, etc).
    # Valid values: html, text.
    format = html
    
    ###########################################
    #editing below this marker not recommended#
    ###########################################
    
    fields=message=%(message)s
      color=%(color)s
      message_format=%(format)s
    url=http://api.hipchat.com/v2/room/%(room_id)s/notification?auth_token=%(auth_token)s
    post_type=json
    
  
  posturl.conf: |
    [posturl]
    enabled=0
    
    # levels can be comma delimited list of any of the following:
    # DEBUG,INFO,WARN,ERROR,CRITICAL,ALERT
    # If left empty, will listen for all levels
    levels=
    
    # clusters is a comma delimited list of cluster names for which
    # this alert config will be eligible to run.
    # If left empty, this alert will will be called for events on all clusters
    clusters=
    
    # the URL to send a HTTP POST to
    url=http://host/path/to/script
    
    # Set a username for basic HTTP authorization
    #username=foo
    
    # Set a password for basic HTTP authorization
    #password=bar
    
    # Override any headers with comma seperated, key=value pairs
    # headers=token=abc123, content-type=application/json
    
    # The number of seconds before a connection to url or proxy_url
    # will be aborted due to a timeout. Defaults to 5 seconds.
    connection_timeout=5
    
    # The number of seconds to wait after sending the POST request before
    # the request will be aborted due to a timeout. Defaults to the same
    # value as connection_timeout if not set.
    #request_timeout=5
    
    # Set the type of posted data. Available options are 'json' or 'form'
    post_type=form
    
    # Fields specified here will override the default event data fields.
    #
    # They must be formatted as key-value pair, with key and value separated by
    # an equals (=). Each pair after the first must be on its own line,
    # indented beyond the first line
    #
    # You may use tokens found within the default event data for or in
    # values. For example, some available keys are:
    #   cluster, time, level_str, message, target_node, event_source, success, api_source_ip, user, source_node
    # Keys must be encapsulated in {brackets}.
    #
    #fields=textKey=value
    #    mixedKey=cluster-{cluster}
    #    event-msg={message}
    
    # Customizable templates for http post data. Some available keys are:
    #     node, cluster, datetime, level_str, message, target_node, event_source, success, api_source_ip, user, source_node
    # More advanced formatting options explained here: https://docs.python.org/2/library/string.html#formatspec Each line
    # beyond the first must be indented. Use with post_type=template, and content-type and other headers must be manually
    # assigned.
    #template={{
    #  "messages":[
    #    {{
    #      "type": "link",
    #      "message": "Cluster {cluster} failure - {message}"
    #      "href": "http://opscenter1"
    #    }}
    #  ]}}
    
  
  snmp.conf: |
    [snmp]
    # set to 1 to enable SNMP trap sending
    enabled=0
    
    # Levels can be a comma-delimited list of any of the following:
    # DEBUG,INFO,WARN,ERROR,CRITICAL,ALERT
    # If the left is empty, OpsCenter will listen for all levels.
    levels=
    
    # Comma-delimited list of cluster names for which
    # this alert config will be eligible to run.
    # If left empty, this alert will be called for events on all clusters.
    clusters=
    
    # SNMP engine ID, specified by rfc3411 and rfc5343.
    # See http://tools.ietf.org/html/rfc3411#section-5
    # SnmpEngineID definition for more information.
    #
    # 32 octet (max length) unique hex engine ID. Must not be all zeroes or all
    # 255's. The first four octets specify the enterprise ID, left filled
    # with zeroes and starting with an 8. The fifth octet specifies a format scheme
    # that specifies the nature of the remaining octets. The remaining octets
    # are given in accordance with the specified format.
    #
    # Format Schemes:
    # 1 -- IPv4 Address scheme
    # 2 -- IPv6 Address scheme
    # 3 -- MAC Address scheme
    # 4 -- Text Address scheme
    # 5 -- Octets scheme
    #
    # Default scheme is octets scheme; if nothing else, you should change
    # 01020304 to a unique octet string.
    #engine_id=80:00:00:00:05:01:02:03:04
    
    # IPv4 address of the SNMP target.
    target_ip=127.0.0.1
    
    # Port to direct traps to on the SNMP target.
    target_port=162
    
    # Set to 1 to use SNMPv3 and the user/privacy key/auth key model. Set to 0 to
    # use SNMPv1/community model.
    use_snmpv3=0
    
    # SNMPv1/2 community name (for community security model)
    community_name=public
    
    # SNMPv3 username
    #user=opscusername
    
    # SNMPv3 authentication protocol
    # Options:
    #   MD5 -- MD5-based authentication protocol
    #   SHA -- SHA-based authentication protocol
    #   NoAuth -- no authentication to use
    #auth_protocol=SHA
    
    # SNMPv3 authentication key
    #auth_key=authkey1
    
    # SNMPv3 privacy protocol
    # Options:
    #   DES -- DES-based encryption protocol
    #   AES -- AES128-based encryption protocol (RFC3826)
    #   3DES -- triple DES-based encryption protocol (Extended Security Options)
    #   AES192 -- AES192-based encryption protocol (Extended Security Options)
    #   AES256 -- AES256-based encryption protocol (Extended Security Options)
    #   NoPriv-- no encryption to use
    #privacy_protocol=AES
    
    # SNMPv3 privacy key
    #privacy_key=privkey1
    
  
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dse-dse-server-opsc-ssl-configmap
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
data:

  
  opscenter.key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQC6V2RD5yeSiSnY
    61CePFkM+ayZXgYnNexpPKpBaPNNczG6L8XHrOf5mG/fqdaYuak1dXq/KtD8Wtw8
    aShbciyDf9imPJOI6OjNzHEU7gIKeBTHg3CvXYtyjebFnZ/urjY5zrk7rsR5kRzy
    CK+cncEs9h9o8Hh4/ach61tNTZbwtYkB/k9uNF/3r9teLXNqcOn8IRh5cTbYLhD6
    vNAxC7Igsvd6nvCpCLkoqWWLM6YqdvUCo5VUatgillbKQ6mcLMSPq4lRoydlyEx2
    kaB9e7E3hr8xp7K+bEXRocknek9n13Qqykf6grF2ZqsW1pj52KqDQYd4y9lIZTP4
    H5yLaYwvAgMBAAECggEAKUSFbgxZVTiPTfz/Q6v1xMvej7ClCDYG/19yDT5i87oZ
    PzSpRoliboJ19EC2x+gGM1KrNn8S/XBZ6f8X8vicJxs/LcGoot+B/2HRgWMcjJj/
    cRgI8q1y3pHSk91OuBtY4wCHQVJ5LLsYf/mweqztRTnUxvbKl3EFfJuVAs+2J+SL
    jCoFjhORC9a7vxhNZKjVEBoM1U+Jl1SbT+jxvE8BDAoPyYtLqrwZ3QHqpnS+SQxl
    5YW91Ps+l/ehTKHoP6UDdqVs6Gtp/APNDT4HrjlyAc3EWy7qFLdzNPEsLH3TbfvW
    0Mp/jj8tn0H2XtYOugQevXBv91EOAvVLGUsd+0YuEQKBgQDd9qKwv/jX0CdZTFJ3
    dQ/ACy+GBwR55vsvAjSppX7a/XBIZ4UtPuzMFybxBPiSvtCgtwpmdEb3r25YRV2c
    X/tyitn5ntAMcUmdgEePIeayX7v0ckWsuyg6kY6jeVpkbG8QRaY4lWwSSX30T1xr
    QsTk1fqc3ZGHlI5w9Qm4XnEFQwKBgQDW6mHVW2tYwOsc1HBDzKGnXf+PbKsOKwxi
    izTDgKMBWFTsvphHK76lHtkQgm5+4DDFo7CMIh+Cjuoq9m6FJaoEK4lJv5bGlZH1
    9G+V4uHJsd/HTxyBxLqt8Aa10zM3JZxxpEel1ch8s+swnAJjVXPsB6tbxurIcTba
    dd7oiwO4pQKBgB3SNhdzI1azc6XuuCGui5gsqIzkQniyTMsLC9JDFuELLdu8dMsY
    ai4dajD1jwQAOP1uuin7s1Exs6yOXT5WMu03tINyD9esHMzZYEPdtSX/8Lut4qiL
    WKNNzREvi7J0UxOnl+aARM3MK+z4AJ20kcmA6jPLHPNPRTft5AluPtcLAoGALJay
    N47hBaf436FfrWFeRJgm9n3IO4dI6YUSLGGa8COAcyHT/CgiRKXSyS8Wd7JRzB96
    +If326GFCebY5Tz8OiNCN4/NW2QaxVmycPmCeNMzp2lNc8r6JC5BZiXsYUunWpiH
    2OsyzlSRviMQB8Xp6BVrQv5RJcb/Z3G/DqYQ3pUCgYATYVG3IBzB4qK/6TpIzcfL
    HbW0hWnffiN7MndbYKslXDaPrUhAQa+I/AGq0ceQbX6g4jA6o4MLRufwlAkZk3tb
    XPTmPG0ZsE2QeVWI0e+oqHQe3fQeGpOSUf/+DLL6sNEU5eE6VhN09skYQ2LpLYTn
    5wX3dYEWl3TqpWe8yMTtCQ==
    -----END PRIVATE KEY-----
    
  
  opscenter.pem: |
    -----BEGIN CERTIFICATE-----
    MIIDzDCCArQCCQCo8TlLba8EOzANBgkqhkiG9w0BAQsFADCBpzELMAkGA1UEBhMC
    VVMxCzAJBgNVBAgMAlRYMQ8wDQYDVQQHDAZBdXN0aW4xGDAWBgNVBAoMD0V4YW1w
    bGUgQ29tcGFueTEZMBcGA1UECwwQRXhhbXBsZSBEaXZpc2lvbjEZMBcGA1UEAwwQ
    WW91ciBDb21tb24gTmFtZTEqMCgGCSqGSIb3DQEJARYbb3BzY2VudGVyLmFkbWlu
    QGV4YW1wbGUuY29tMB4XDTE4MDgxMDE5MjIxMFoXDTI4MDgwNzE5MjIxMFowgacx
    CzAJBgNVBAYTAlVTMQswCQYDVQQIDAJUWDEPMA0GA1UEBwwGQXVzdGluMRgwFgYD
    VQQKDA9FeGFtcGxlIENvbXBhbnkxGTAXBgNVBAsMEEV4YW1wbGUgRGl2aXNpb24x
    GTAXBgNVBAMMEFlvdXIgQ29tbW9uIE5hbWUxKjAoBgkqhkiG9w0BCQEWG29wc2Nl
    bnRlci5hZG1pbkBleGFtcGxlLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
    AQoCggEBALpXZEPnJ5KJKdjrUJ48WQz5rJleBic17Gk8qkFo801zMbovxces5/mY
    b9+p1pi5qTV1er8q0Pxa3DxpKFtyLIN/2KY8k4jo6M3McRTuAgp4FMeDcK9di3KN
    5sWdn+6uNjnOuTuuxHmRHPIIr5ydwSz2H2jweHj9pyHrW01NlvC1iQH+T240X/ev
    214tc2pw6fwhGHlxNtguEPq80DELsiCy93qe8KkIuSipZYszpip29QKjlVRq2CKW
    VspDqZwsxI+riVGjJ2XITHaRoH17sTeGvzGnsr5sRdGhySd6T2fXdCrKR/qCsXZm
    qxbWmPnYqoNBh3jL2UhlM/gfnItpjC8CAwEAATANBgkqhkiG9w0BAQsFAAOCAQEA
    jaYVgLmIhLIpo9w/jQLuMX4YMEwedngacm3UU3WrgF1Df7DPXhInoa9hWQegPX94
    wiDhA5f6l54As5k3o3EzMn2PZVU5hRcjS3pKXD+VEElNqh6A2i6dJHNgoUj3c1Ai
    YtIJZTvCXumVQ8EspzexUYEhnmqA6mC9kTZV2/jc0AahsffGrMOx17EZ2ZFkhZUY
    N6ihxRNcKWdQXM1GU5jPd9gk2KbFT7Z6DqHrAb+eDGq5AWAo3XjLsCR+T852yLx1
    UWhKly7ARWkowv8n9NrGIXJbX8o0+jzrqZDmU2WHKnLv6xFRUWJASoZdp+kPVRsV
    S4fXLdEEeJpkByGWj6QyvQ==
    -----END CERTIFICATE-----
    
  
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dse-dse-server-opsc-conf-configmap
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
data:

  
  logback.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <!--
    Logback configuration file for OpsCenter.
    
    Common options that you may want to change include:
    
        file - This is the name and location of the active log file that is currently
            being written to. This maps to the log_path property in previous versions
            of OpsCenter. If you change this property, you may want to also change
            fileNamePattern.
    
        fileNamePattern - This is the name, location and pattern of log files after they
            exceed the rolling policy. If you change this property, you may want to also
            change file.
    
        maxIndex - This is the number of rolled log files to keep. This maps to the max_rotate
            property in previous versions of OpsCenter. The default value is 10.
    
        maxFileSize - This is the file size that will cause the current log file to
            roll into an archived file. This maps to the log_length property in previous
            versions of OpsCenter. The default is '10MB'.
    
        level - This is the minimum logging level that will be included in the log
            files along with all higher logging levels. Valid values are TRACE, DEBUG,
            INFO, WARN and ERROR. Unlike previous versions of OpsCenter logging, each
            logger can have a different level associated with it. Changing the level
            property on the <root> element is equivalent to setting the level property
            in previous versions of OpsCenter.
    
    Additional details on advanced configuration options can be found in the Logback
    manual at  http://logback.qos.ch/manual/configuration.html.
    -->
    <configuration>
        <appender name="opscenterd_log" class="ch.qos.logback.core.rolling.RollingFileAppender">
          <file>./log/opscenterd.log</file>
            <encoder>
                <charset>UTF-8</charset>
                <pattern>%date{ISO8601, UTC} [%X{cluster_id:-opscenterd}] %5level: %msg \(%thread\)%n%exception{20}</pattern>
            </encoder>
            <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
              <fileNamePattern>./log/opscenterd.%i.log</fileNamePattern>
              <minIndex>1</minIndex>
              <maxIndex>10</maxIndex>
            </rollingPolicy>
            <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                <maxFileSize>10MB</maxFileSize>
            </triggeringPolicy>
        </appender>
    
        <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
            <encoder>
                <charset>UTF-8</charset>
                <pattern>%date{ISO8601, UTC} [%X{cluster_id:-opscenterd}] %5level: %msg \(%thread\)%n%exception{20}</pattern>
            </encoder>
            <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
                <level>INFO</level>
            </filter>
        </appender>
    
        <appender name="repair_log" class="ch.qos.logback.classic.sift.SiftingAppender">
          <discriminator>
              <key>cluster_id</key>
              <defaultValue>unknown</defaultValue>
          </discriminator>
          <sift>
            <appender name="repair_log_${cluster_id}" class="ch.qos.logback.core.rolling.RollingFileAppender">
              <file>./log/repair_service/${cluster_id}.log</file>
                <encoder>
                    <charset>UTF-8</charset>
                    <pattern>%date{ISO8601, UTC} [%X{repair_type:-repair_service}] %5level: %msg \(%thread\)%n%exception{20}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
                  <fileNamePattern>./log/repair_service/${cluster_id}.%i.log</fileNamePattern>
                  <minIndex>1</minIndex>
                  <maxIndex>10</maxIndex>
                </rollingPolicy>
                <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                    <maxFileSize>10MB</maxFileSize>
                </triggeringPolicy>
                <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
                    <level>INFO</level>
                </filter>
            </appender>
          </sift>
        </appender>
    
        <appender name="http_log" class="ch.qos.logback.core.rolling.RollingFileAppender">
          <file>./log/http.log</file>
            <encoder>
                <charset>UTF-8</charset>
                <pattern>%date{ISO8601, UTC} [%X{cluster_id}] %5level: %msg \(%thread\)%n%exception{20}</pattern>
            </encoder>
            <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
              <fileNamePattern>./log/http.%i.log</fileNamePattern>
              <minIndex>1</minIndex>
              <maxIndex>10</maxIndex>
            </rollingPolicy>
            <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                <maxFileSize>10MB</maxFileSize>
            </triggeringPolicy>
            <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
                <level>INFO</level>
            </filter>
        </appender>
    
        <appender name="security" class="ch.qos.logback.core.ConsoleAppender">
            <encoder>
                <charset>UTF-8</charset>
                <pattern>%date{ISO8601, UTC} [%X{cluster_id}] %msg \(%thread\)%n%exception{20}</pattern>
            </encoder>
            <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
                <level>INFO</level>
            </filter>
        </appender>
    
        <root level="INFO">
            <appender-ref ref="opscenterd_log"/>
            <appender-ref ref="STDOUT"/>
        </root>
    
        <logger name="com.datastax.driver" level="WARN" additivity="false"/>
        <logger name="com.datastax.driver.core.FrameCompressor" level="ERROR"/>
        <logger name="org.apache.mina" level="INFO" additivity="false" />
        <logger name="org.apache.directory" level="INFO" additivity="false"/>
        <logger name="org.python" level="ERROR"/>
        <logger name="org.jboss.netty" level="ERROR"/>
        <logger name="org.apache.http" level="ERROR"/>
        <logger name="com.mchange" level="ERROR"/>
        <logger name="lcm" level="INFO"/>
        <logger name="lcm.database.migration" level="WARN"/>
        <logger name="io.netty.util.concurrent.DefaultPromise.rejectedExecution" level="DEBUG" />
    
        <!-- Repair Service logger -->
        <logger name="opscenterd.repair" additivity="false">
          <appender-ref ref="repair_log"/>
        </logger>
    
        <!-- HTTP Request logger -->
        <logger name="opscenterd.http" additivity="false">
          <appender-ref ref="http_log"/>
        </logger>
    </configuration>
    
  
  opscenterd.conf: |
    # opscenterd.conf
    
    [webserver]
    port = 8888
    interface = 0.0.0.0
    # The following settings can be used to enable ssl support for the opscenter
    # web application. Change these values to point to the ssl certificate and key
    # that you wish to use for your OpsCenter install, as well as the port you would like
    # to serve ssl traffic from.
    ssl_keyfile = /var/lib/opscenter/ssl/opscenter.key
    ssl_certfile = /var/lib/opscenter/ssl/opscenter.pem
    ssl_port = 8443
    
    [authentication]
    # Set this option to True to enable OpsCenter authentication.  A default admin
    # account will be created with the username "admin" and password "admin".
    # Accounts and roles can then be created and modified from within the web UI.
    enabled = True
    
    # To help us better understand the needs of users and to improve OpsCenter, OpsCenter
    # reports information about itself and the clusters it manages to a central DataStax
    # server.  This information is reported anonymously, and potentially sensitive
    # information, such as IP addresses, are hashed in a non-reversible way:
    # http://www.datastax.com/documentation/opscenter/help/statsReporterProperties.html
    [stat_reporter]
    # The interval setting determines how often statistics are reported.  To disable
    # reporting, set to 0
    # interval = 86400 # 24 hours
    
  
  ssl.conf: |
    [ req ]
    prompt=no
    distinguished_name = req_distinguished_name
    
    [ req_distinguished_name ]
    C = US
    ST = TX
    L = Austin
    O = Example Company
    OU = Example Division
    CN = Your Common Name
    emailAddress = opscenter.admin@example.com
    
  
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dse-dse-server-cassandra-conf-configmap
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
data:

  
  cassandra-env.sh: |
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    calculate_system_memory_sizes()
    {
        if [ "$system_memory_sizes_calculated" = true ] ; then
            return
        fi
    
        case "`uname`" in
            Linux)
                system_memory_in_mb=`free -m | awk '/:/ {print $2;exit}'`
                system_cpu_cores=`egrep -c 'processor([[:space:]]+):.*' /proc/cpuinfo`
            ;;
            FreeBSD)
                system_memory_in_bytes=`sysctl hw.physmem | awk '{print $2}'`
                system_memory_in_mb=`expr $system_memory_in_bytes / 1024 / 1024`
                system_cpu_cores=`sysctl hw.ncpu | awk '{print $2}'`
            ;;
            SunOS)
                system_memory_in_mb=`prtconf | awk '/Memory size:/ {print $3}'`
                system_cpu_cores=`psrinfo | wc -l`
            ;;
            Darwin)
                system_memory_in_bytes=`sysctl hw.memsize | awk '{print $2}'`
                system_memory_in_mb=`expr $system_memory_in_bytes / 1024 / 1024`
                system_cpu_cores=`sysctl hw.ncpu | awk '{print $2}'`
            ;;
            *)
                # assume reasonable defaults for e.g. a modern desktop or
                # cheap server
                system_memory_in_mb="2048"
                system_cpu_cores="2"
            ;;
        esac
    
        # some systems like the raspberry pi don't report cores, use at least 1
        if [ "$system_cpu_cores" -lt "1" ]
        then
            system_cpu_cores="1"
        fi
    
        # cap here to 32765M because the JVM switches to 64 bit references at 32767M
        # details are described in http://java-performance.info/over-32g-heap-java/
        capped_heap_size="32765"
    
        # set max heap size based on the following
        # max(min(1/2 ram, 1024MB), min(1/4 ram, 8GB))
        # calculate 1/2 ram and cap to 1024MB
        # calculate 1/4 ram and cap to capped_heap_size
        # pick the max
    
        # set max heap size by calculating 1/2 ram and capping to 32Gb
        half_system_memory_in_mb=`expr $system_memory_in_mb / 2`
        quarter_system_memory_in_mb=`expr $half_system_memory_in_mb / 2`
        if [ "$half_system_memory_in_mb" -gt "1024" ]
        then
            half_system_memory_in_mb="1024"
        fi
        if [ "$quarter_system_memory_in_mb" -gt "$capped_heap_size" ]
        then
            quarter_system_memory_in_mb="$capped_heap_size"
        fi
        if [ "$half_system_memory_in_mb" -gt "$quarter_system_memory_in_mb" ]
        then
            max_heap_size_in_mb="$half_system_memory_in_mb"
        else
            max_heap_size_in_mb="$quarter_system_memory_in_mb"
        fi
    
        if [ "$JVM_VENDOR" = "Azul" ]; then
            # DSP-14197: round it down to the next even number as the Zing JDK has a bug were -Xmx numbers might be rounded
            # down and we would end up with -Xms being 1 larger than -Xmx and startup would fail.
            max_heap_size_in_mb=$(( ${max_heap_size_in_mb} - (${max_heap_size_in_mb} % 2) ))
        fi
        system_memory_sizes_calculated=true
    }
    
    calculate_system_memory_sizes
    
    calculate_heap_sizes()
    {
        MAX_HEAP_SIZE="${max_heap_size_in_mb}M"
    
        # Young gen: min(max_sensible_per_modern_cpu_core * num_cores, 1/4 * heap size)
        max_sensible_yg_per_core_in_mb="100"
        max_sensible_yg_in_mb=`expr $max_sensible_yg_per_core_in_mb "*" $system_cpu_cores`
    
        desired_yg_in_mb=`expr $max_heap_size_in_mb / 4`
    
        if [ "$desired_yg_in_mb" -gt "$max_sensible_yg_in_mb" ]
        then
            HEAP_NEWSIZE="${max_sensible_yg_in_mb}M"
        else
            HEAP_NEWSIZE="${desired_yg_in_mb}M"
        fi
    }
    
    # Determine the sort of JVM we'll be running on.
    java_ver_output=`"${JAVA:-java}" -version 2>&1`
    jvmver=`echo "$java_ver_output" | grep '[openjdk|java] version' | awk -F'"' 'NR==1 {print $2}' | cut -d\- -f1`
    JVM_VERSION=${jvmver%_*}
    JVM_PATCH_VERSION=${jvmver#*_}
    
    if [ "$JVM_VERSION" \< "1.8" ] || [ "$JVM_VERSION" \> "1.8.2" ] ; then
        echo "DSE 6.0 requires Java 8 update 151 or later. Java $JVM_VERSION is not supported."
        exit 1;
    fi
    
    if [ "$JVM_PATCH_VERSION" -lt 151 ] ; then
        echo "DSE 6.0 requires Java 8 update 151 or later. Java 8 update $JVM_PATCH_VERSION is not supported."
        exit 1;
    fi
    
    jvm=`echo "$java_ver_output" | grep -A 1 '[openjdk|java] version' | awk 'NR==2 {print $1}'`
    case "$jvm" in
        OpenJDK)
            JVM_VENDOR=OpenJDK
            # this will be "64-Bit" or "32-Bit"
            JVM_ARCH=`echo "$java_ver_output" | awk 'NR==3 {print $2}'`
            ;;
        "Java(TM)")
            JVM_VENDOR=Oracle
            # this will be "64-Bit" or "32-Bit"
            JVM_ARCH=`echo "$java_ver_output" | awk 'NR==3 {print $3}'`
            ;;
        "Zing")
            JVM_VENDOR=Azul
            # this will be "64-Bit" or "32-Bit"
            JVM_ARCH=`echo "$java_ver_output" | awk 'NR==3 {print $2}'`
            ;;
        *)
            # Help fill in other JVM values
            JVM_VENDOR=other
            JVM_ARCH=unknown
            ;;
    esac
    
    #GC log path has to be defined here because it needs to access CASSANDRA_HOME
    JVM_OPTS="$JVM_OPTS -Xloggc:${CASSANDRA_LOG_DIR}/gc.log"
    
    # Here we create the arguments that will get passed to the jvm when
    # starting cassandra.
    
    # Read user-defined JVM options from jvm.options file
    JVM_OPTS_FILE=$CASSANDRA_CONF/jvm.options
    for opt in `grep "^-" $JVM_OPTS_FILE`
    do
      JVM_OPTS="$JVM_OPTS $opt"
    done
    
    # Check what parameters were defined on jvm.options file to avoid conflicts
    echo $JVM_OPTS | egrep -q "(^|\s)-Xmn"
    DEFINED_XMN=$?
    echo $JVM_OPTS | egrep -q "(^|\s)-Xmx"
    DEFINED_XMX=$?
    echo $JVM_OPTS | egrep -q "(^|\s)-Xms"
    DEFINED_XMS=$?
    echo $JVM_OPTS | egrep -q "(^|\s)-XX:\+UseConcMarkSweepGC"
    USING_CMS=$?
    echo $JVM_OPTS | egrep -q "(^|\s)-XX:\+UseG1GC"
    USING_G1=$?
    
    # Override these to set the amount of memory to allocate to the JVM at
    # start-up. For production use you may wish to adjust this for your
    # environment. MAX_HEAP_SIZE is the total amount of memory dedicated
    # to the Java heap. HEAP_NEWSIZE refers to the size of the young
    # generation. Both MAX_HEAP_SIZE and HEAP_NEWSIZE should be either set
    # or not (if you set one, set the other).
    #
    # The main trade-off for the young generation is that the larger it
    # is, the longer GC pause times will be. The shorter it is, the more
    # expensive GC will be (usually).
    #
    # The example HEAP_NEWSIZE assumes a modern 8-core+ machine for decent pause
    # times. If in doubt, and if you do not particularly want to tweak, go with
    # 100 MB per physical CPU core.
    
    #MAX_HEAP_SIZE="4G"
    #HEAP_NEWSIZE="800M"
    
    # Set this to control the amount of arenas per-thread in glibc
    #export MALLOC_ARENA_MAX=4
    
    # only calculate the size if it's not set manually
    if [ "x$MAX_HEAP_SIZE" = "x" ] && [ "x$HEAP_NEWSIZE" = "x" -o $USING_G1 -eq 0 ]; then
        calculate_heap_sizes
    elif [ "x$MAX_HEAP_SIZE" = "x" ] ||  [ "x$HEAP_NEWSIZE" = "x" -a $USING_G1 -ne 0 ]; then
        echo "please set or unset MAX_HEAP_SIZE and HEAP_NEWSIZE in pairs when using CMS GC (see cassandra-env.sh)"
        exit 1
    fi
    
    heap_size_in_mb=0
    if echo "$MAX_HEAP_SIZE" | grep -qi "G$" ;
    then
        heap_size_in_mb="$((${MAX_HEAP_SIZE%?} * 1024))"
    elif  echo "$MAX_HEAP_SIZE" | grep -qi "M$" ;
    then
        heap_size_in_mb="${MAX_HEAP_SIZE%?}"
    elif  echo "$MAX_HEAP_SIZE" | grep -qi "K$" ;
    then
        heap_size_in_mb="$((${MAX_HEAP_SIZE%?} / 1024))"
    fi
    
    memory_remaining_in_mb="$((${system_memory_in_mb} - ${heap_size_in_mb}))"
    
    # Calculate direct memory as 1/2 of memory available after heap:
    MAX_DIRECT_MEMORY="$((memory_remaining_in_mb / 2))M"
    
    JVM_OPTS="$JVM_OPTS -XX:MaxDirectMemorySize=$MAX_DIRECT_MEMORY"
    
    if [ "x$MALLOC_ARENA_MAX" = "x" ] ; then
        export MALLOC_ARENA_MAX=4
    fi
    
    # We only set -Xms and -Xmx if they were not defined on jvm.options file
    # If defined, both Xmx and Xms should be defined together.
    if [ $DEFINED_XMX -ne 0 ] && [ $DEFINED_XMS -ne 0 ]; then
        JVM_OPTS="$JVM_OPTS -Xms${MAX_HEAP_SIZE}"
        JVM_OPTS="$JVM_OPTS -Xmx${MAX_HEAP_SIZE}"
    elif [ $DEFINED_XMX -ne 0 ] || [ $DEFINED_XMS -ne 0 ]; then
        echo "Please set or unset -Xmx and -Xms flags in pairs on jvm.options file."
        exit 1
    fi
    
    # We only set -Xmn flag if it was not defined in jvm.options file
    # and if the CMS GC is being used
    # If defined, both Xmn and Xmx should be defined together.
    if [ $DEFINED_XMN -eq 0 ] && [ $DEFINED_XMX -ne 0 ]; then
        echo "Please set or unset -Xmx and -Xmn flags in pairs on jvm.options file."
        exit 1
    elif [ $DEFINED_XMN -ne 0 ] && [ $USING_CMS -eq 0 ]; then
        JVM_OPTS="$JVM_OPTS -Xmn${HEAP_NEWSIZE}"
    fi
    
    if [ "$JVM_ARCH" = "64-Bit" ] && [ $USING_CMS -eq 0 ]; then
        JVM_OPTS="$JVM_OPTS -XX:+UseCondCardMark"
    fi
    
    # provides hints to the JIT compiler
    JVM_OPTS="$JVM_OPTS -XX:CompileCommandFile=$CASSANDRA_CONF/hotspot_compiler"
    
    # add the jamm javaagent
    JVM_OPTS="$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.3.0.jar"
    
    # set jvm HeapDumpPath with CASSANDRA_HEAPDUMP_DIR
    if [ "x$CASSANDRA_HEAPDUMP_DIR" != "x" ]; then
        JVM_OPTS="$JVM_OPTS -XX:HeapDumpPath=$CASSANDRA_HEAPDUMP_DIR/cassandra-`date +%s`-pid$$.hprof"
    fi
    
    # stop the jvm on OutOfMemoryError as it can result in some data corruption
    # uncomment the preferred option
    # ExitOnOutOfMemoryError and CrashOnOutOfMemoryError require a JRE greater or equals to 1.7 update 101 or 1.8 update 92
    # For OnOutOfMemoryError we cannot use the JVM_OPTS variables because bash commands split words
    # on white spaces without taking quotes into account
    # JVM_OPTS="$JVM_OPTS -XX:+ExitOnOutOfMemoryError"
    # JVM_OPTS="$JVM_OPTS -XX:+CrashOnOutOfMemoryError"
    JVM_ON_OUT_OF_MEMORY_ERROR_OPT="-XX:OnOutOfMemoryError=kill -9 %p"
    
    # print an heap histogram on OutOfMemoryError
    # JVM_OPTS="$JVM_OPTS -Dcassandra.printHeapHistogramOnOutOfMemoryError=true"
    
    # jmx: metrics and administration interface
    #
    # add this if you're having trouble connecting:
    # JVM_OPTS="$JVM_OPTS -Djava.rmi.server.hostname=<public name>"
    #
    # see
    # https://blogs.oracle.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole
    # for more on configuring JMX through firewalls, etc. (Short version:
    # get it working with no firewall first.)
    #
    # Cassandra ships with JMX accessible *only* from localhost.
    # To enable remote JMX connections, uncomment lines below
    # with authentication and/or ssl enabled. See https://wiki.apache.org/cassandra/JmxSecurity
    #
    if [ "x$LOCAL_JMX" = "x" ]; then
        LOCAL_JMX=yes
    fi
    
    # Specifies the default port over which Cassandra will be available for
    # JMX connections.
    # For security reasons, you should not expose this port to the internet.  Firewall it if needed.
    JMX_PORT="7199"
    
    if [ "$LOCAL_JMX" = "yes" ]; then
      JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.local.port=$JMX_PORT"
      JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false"
    else
      JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.remote.port=$JMX_PORT"
      # if ssl is enabled the same port cannot be used for both jmx and rmi so either
      # pick another value for this property or comment out to use a random port (though see CASSANDRA-7087 for origins)
      JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT"
    
      # turn on JMX authentication. See below for further options
      JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=true"
    
      # jmx ssl options
      #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl=true"
      #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.need.client.auth=true"
      #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.protocols=<enabled-protocols>"
      #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.cipher.suites=<enabled-cipher-suites>"
      #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.keyStore=/path/to/keystore"
      #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.keyStorePassword=<keystore-password>"
      #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.trustStore=/path/to/truststore"
      #JVM_OPTS="$JVM_OPTS -Djavax.net.ssl.trustStorePassword=<truststore-password>"
    fi
    
    # jmx authentication and authorization options. By default, auth is only
    # activated for remote connections but they can also be enabled for local only JMX
    ## Basic file based authn & authz
    JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password"
    #JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.access.file=/etc/cassandra/jmxremote.access"
    ## Custom auth settings which can be used as alternatives to JMX's out of the box auth utilities.
    ## JAAS login modules can be used for authentication by uncommenting these two properties.
    ## Cassandra ships with a LoginModule implementation - org.apache.cassandra.auth.CassandraLoginModule -
    ## which delegates to the IAuthenticator configured in cassandra.yaml. See the sample JAAS configuration
    ## file cassandra-jaas.config
    #JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.remote.login.config=CassandraLogin"
    #JVM_OPTS="$JVM_OPTS -Djava.security.auth.login.config=$CASSANDRA_HOME/conf/cassandra-jaas.config"
    
    ## Cassandra also ships with a helper for delegating JMX authz calls to the configured IAuthorizer,
    ## uncomment this to use it. Requires one of the two authentication options to be enabled
    #JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.authorizer=org.apache.cassandra.auth.jmx.AuthorizationProxy"
    
    # To use mx4j, an HTML interface for JMX, add mx4j-tools.jar to the lib/
    # directory.
    # See http://wiki.apache.org/cassandra/Operations#Monitoring_with_MX4J
    # By default mx4j listens on 0.0.0.0:8081. Uncomment the following lines
    # to control its listen address and port.
    #MX4J_ADDRESS="127.0.0.1"
    #MX4J_PORT="8081"
    
    if [ "x$MX4J_ADDRESS" = "x" ]; then
        # override default of 0.0.0.0 if no MX4J_ADDRESS is specified
        MX4J_ADDRESS="127.0.0.1"
    fi
    if echo "$MX4J_ADDRESS" | grep -qi "\-Dmx4jaddress*" ; then
        # Backward compatible with the older style #13578
        JVM_OPTS="$JVM_OPTS $MX4J_ADDRESS"
    else
        JVM_OPTS="$JVM_OPTS -Dmx4jaddress=$MX4J_ADDRESS"
    fi
    if [ "x$MX4J_PORT" != "x" ]; then
        if echo "$MX4J_PORT" | grep -qi "\-Dmx4jport*" ; then
            # Backward compatible with the older style #13578
            JVM_OPTS="$JVM_OPTS $MX4J_PORT"
        else
            JVM_OPTS="$JVM_OPTS -Dmx4jport=$MX4J_PORT"
        fi
    fi
    
    # Cassandra uses SIGAR to capture OS metrics CASSANDRA-7838
    # for SIGAR we have to set the java.library.path
    # to the location of the native libraries.
    JVM_OPTS="$JVM_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
    
    # We need to expose the available system memory so that the
    # MemoryOnlyStrategy can do proper fraction calculations.
    # See max_memory_to_lock_fraction setting in cassandra.yaml for details.
    JVM_OPTS="$JVM_OPTS -Ddse.system_memory_in_mb=$system_memory_in_mb"
    
    # Disable Agrona bounds check for extra performance
    JVM_OPTS="$JVM_OPTS -Dagrona.disable.bounds.checks=TRUE"
    
    JVM_OPTS="$JVM_OPTS $JVM_EXTRA_OPTS"
    
    # add the DSE loader
    JVM_OPTS="$JVM_OPTS $DSE_OPTS"
    
    
  
  cassandra-rackdc.properties: |
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    # These properties are used with GossipingPropertyFileSnitch and will
    # indicate the rack and dc for this node
    dc=dc1
    rack=rack1
    
    # Add a suffix to a datacenter name. Used by the Ec2Snitch and Ec2MultiRegionSnitch
    # to append a string to the EC2 region name.
    #dc_suffix=
    
    # Uncomment the following line to make this snitch prefer the internal ip when possible, as the Ec2MultiRegionSnitch does.
    # prefer_local=true
    
  
  cassandra.yaml: |
    # cassandra.yaml is the main storage configuration file for DataStax Enterprise (DSE). 
    
    # NOTE:
    #   See the DataStax Enterprise documentation at https://docs.datastax.com/
    # /NOTE
    
    # The name of the cluster. This is mainly used to prevent machines in
    # one logical cluster from joining another.
    cluster_name: 'My_Cluster'
    
    # The number of tokens randomly assigned to this node on the ring.
    # The higher the token count is relative to other nodes, the larger the proportion of data
    # that this node will store. You probably want all nodes to have the same number
    # of tokens assuming they have equal hardware capability.
    #
    # If not set, the default value is 1 token for backward compatibility
    # and will use the initial_token as described below.
    #
    # Specifying initial_token will override this setting on the node's initial start.
    # On subsequent starts, this setting will apply even if initial token is set.
    #
    # If you already have a cluster with 1 token per node, and want to migrate to
    # multiple tokens per node, see http://wiki.apache.org/cassandra/Operations
    num_tokens: 128
    
    # Triggers automatic allocation of num_tokens tokens for this node. The allocation
    # algorithm attempts to choose tokens in a way that optimizes replicated load over
    # the nodes in the datacenter for the specified DC-level replication factor.
    #
    # The load assigned to each node will be close to proportional to its number of
    # vnodes.
    #
    # Supported only with the Murmur3Partitioner.
    # allocate_tokens_for_local_replication_factor: 3
    
    # initial_token allows you to specify tokens manually.  To use with
    # vnodes (num_tokens > 1, above), provide a
    # comma-separated list of tokens. This option is primarily used when adding nodes to legacy clusters
    # that do not have vnodes enabled.
    # initial_token:
    
    # See http://wiki.apache.org/cassandra/HintedHandoff
    # True to enable globally, false to disable globally.
    hinted_handoff_enabled: true
    
    # When hinted_handoff_enabled is true, a black list of data centers that will not
    # perform hinted handoff. Other datacenters not listed will perform hinted handoffs.
    # hinted_handoff_disabled_datacenters:
    #    - DC1
    #    - DC2
    
    # Maximum amount of time during which the database generates hints for an unresponsive node.
    # After this interval, the database does not generate any new hints for the node until it is
    # back up and responsive.  If the node goes down again, the database starts a new interval. This setting
    # can prevent a sudden demand for resources when a node is brought back online and the rest of the
    # cluster attempts to replay a large volume of hinted writes.
    max_hint_window_in_ms: 10800000 # 3 hours
    
    # Maximum throttle in KBs per second per delivery thread.  This will be
    # reduced proportionally to the number of nodes in the cluster.  If there
    # are two nodes in the cluster, each delivery thread will use the maximum
    # rate; if there are three, each will throttle to half of the maximum,
    # since we expect two nodes to be delivering hints simultaneously.
    hinted_handoff_throttle_in_kb: 1024
    
    # Number of threads with which to deliver hints;
    # Consider increasing this number when you have multi-dc deployments, since
    # cross-dc handoff tends to be slower
    max_hints_delivery_threads: 2
    
    # Directory to store hints.
    # If not set, the default directory is $DSE_HOME/data/hints.
    hints_directory: /var/lib/cassandra/hints
    
    # How often to flush hints from the internal buffers to disk.
    # Will *not* trigger fsync.
    hints_flush_period_in_ms: 10000
    
    # Maximum size, in MB, for a single hints file.
    max_hints_file_size_in_mb: 128
    
    # Compression to apply to the hint files. If omitted, hints files
    # will be written uncompressed. LZ4, Snappy, and Deflate compressors
    # are supported.
    #hints_compression:
    #   - class_name: LZ4Compressor
    #     parameters:
    #         -
    
    # Maximum throttle in KBs per second, total. This will be
    # reduced proportionally to the number of nodes in the cluster.
    batchlog_replay_throttle_in_kb: 1024
    
    # Strategy to choose the batchlog storage endpoints.
    #
    # Available options:
    #
    # - random_remote
    #   Default, purely random. Prevents the local rack, if possible. Same behavior as earlier releases.
    #
    # - dynamic_remote
    #   Uses DynamicEndpointSnitch to select batchlog storage endpoints. Prevents the
    #   local rack, if possible. This strategy offers the same availability guarantees
    #   as random_remote, but selects the fastest endpoints according to the DynamicEndpointSnitch.
    #   (DynamicEndpointSnitch tracks reads but not writes. Write-only,
    #   or mostly-write, workloads might not benefit from this strategy.
    #   Note: this strategy will fall back to random_remote if dynamic_snitch is not enabled.
    #
    # - dynamic
    #   Mostly the same as dynamic_remote, except that local rack is not excluded, which offers lower
    #   availability guarantee than random_remote or dynamic_remote.
    #   Note: this strategy will fall back to random_remote if dynamic_snitch is not enabled.
    #
    # batchlog_endpoint_strategy: random_remote
    
    # DataStax Enterprise (DSE) provides the DseAuthenticator for external authentication
    # with multiple authentication schemes such as Kerberos, LDAP, and internal authentication.
    # Additional configuration is required in dse.yaml for enabling authentication.
    # If using DseAuthenticator, DseRoleManager must also be used (see below).
    #
    # All other authenticators, including org.apache.cassandra.auth.{AllowAllAuthenticator,
    # PasswordAuthenticator} are deprecated, and some security features may not work
    # correctly if they are used.
    authenticator: com.datastax.bdp.cassandra.auth.DseAuthenticator
    
    # DataStax Enterprise (DSE) provides the DseAuthorizer which must be used in place
    # of the CassandraAuthorizer if the DseAuthenticator is being used. It allows
    # enhanced permission management of DSE specific resources.
    # Additional configuration is required in dse.yaml for enabling authorization.
    #
    # All other authorizers, including org.apache.cassandra.auth.{AllowAllAuthorizer,
    # CassandraAuthorizer} are deprecated, and some security features may not work
    # correctly if they are used.
    authorizer: com.datastax.bdp.cassandra.auth.DseAuthorizer
    
    # DataStax Enterprise (DSE) provides the DseRoleManager that supports LDAP roles
    # as well as the internal roles supported by CassandraRoleManager. The DseRoleManager
    # stores role options in the dse_security keyspace.
    # Please increase the dse_security keyspace replication factor when using this role
    # manager. Additional configuration is required in dse.yaml.
    #
    # All other role managers, including CassandraRoleManager are deprecated, and some
    # security features might not work correctly if they are used.
    role_manager: com.datastax.bdp.cassandra.auth.DseRoleManager
    
    # Whether to enable system keyspace filtering so that users can access and view
    # only schema information for rows in the system and system_schema keyspaces to
    # which they have access. Security requirements and user permissions apply.
    # Enable this feature only after appropriate user permissions are granted.
    #
    # See Managing keyspace and table permissions at
    # http://http://docsreview.datastax.lan/en/dse/6.0/dse-admin/datastax_enterprise/security/secDataPermission.html
    #
    # Default: false
    system_keyspaces_filtering: false
    
    # Validity period for roles cache (fetching granted roles can be an expensive
    # operation depending on the role manager)
    # Granted roles are cached for authenticated sessions in AuthenticatedUser and
    # after the period specified here, become eligible for (async) reload.
    # Defaults to 120000, set to 0 to disable caching entirely.
    # Will be disabled automatically if internal authentication is disabled
    # when using DseAuthenticator.
    roles_validity_in_ms: 120000
    
    # Refresh interval for roles cache (if enabled).
    # After this interval, cache entries become eligible for refresh. On next
    # access, an async reload is scheduled and returns the old value until the reload
    # completes. If roles_validity_in_ms is non-zero, then this value must be non-zero
    # also.
    # Defaults to the same value as roles_validity_in_ms.
    # roles_update_interval_in_ms: 2000
    
    # Validity period for permissions cache (fetching permissions can be an
    # expensive operation depending on the authorizer).
    # Defaults to 120000, set to 0 to disable.
    # Will be disabled automatically if authorization is disabled when
    # using DseAuthorizer.
    permissions_validity_in_ms: 120000
    
    # Refresh interval for permissions cache (if enabled).
    # After this interval, cache entries become eligible for refresh. Upon next
    # access, an async reload is scheduled and the old value returned until it
    # completes. If permissions_validity_in_ms is non-zero, then this value must also be
    # non-zero.
    # Defaults to the same value as permissions_validity_in_ms.
    # permissions_update_interval_in_ms: 2000
    
    # The partitioner is responsible for distributing groups of rows (by
    # partition key) across nodes in the cluster.  You should leave this
    # alone for new clusters.  The partitioner can NOT be changed without
    # reloading all data, so when upgrading you should set this to the
    # same partitioner you were already using.
    #
    # Besides Murmur3Partitioner, partitioners included for backwards
    # compatibility include RandomPartitioner, ByteOrderedPartitioner, and
    # OrderPreservingPartitioner.
    #
    partitioner: org.apache.cassandra.dht.Murmur3Partitioner
    
    # Directories where the database should store data on disk. The data
    # is spread evenly across the directories, subject to the granularity of
    # the configured compaction strategy.
    # If not set, the default directory is $DSE_HOME/data/data.
    data_file_directories:
         - /var/lib/cassandra/data
    
    # Commit log directory. When running on magnetic HDD, this directory should be on a
    # separate spindle than the data directories.
    # If not set, the default directory is $DSE_HOME/data/commitlog.
    commitlog_directory: /var/lib/cassandra/commitlog
    
    # Whether to enable CDC functionality on a per-node basis. CDC functionality modifies the logic used
    # for write path allocation rejection. When false (standard behavior), never reject. When true (use cdc functionality),
    # reject mutation that contains a CDC-enabled table if at space limit threshold in cdc_raw_directory.
    cdc_enabled: false
    
    # CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the
    # segment contains mutations for a CDC-enabled table. This directory should be placed on a
    # separate spindle than the data directories. If not set, the default directory is
    # $DSE_HOME/data/cdc_raw.
    cdc_raw_directory: /var/lib/cassandra/cdc_raw
    
    # Policy for data disk failures:
    #
    # die
    #   shut down gossip and client transports and kill the JVM for any fs errors or
    #   single-sstable errors, so the node can be replaced.
    #
    # stop_paranoid
    #   shut down gossip and client transports even for single-sstable errors,
    #   kill the JVM for errors during startup.
    #
    # stop
    #   shut down gossip and client transports, leaving the node effectively dead, but
    #   can still be inspected via JMX, kill the JVM for errors during startup.
    #
    # best_effort
    #    stop using the failed disk and respond to requests based on
    #    remaining available sstables.  This means you WILL see obsolete
    #    data at CL.ONE!
    #
    # ignore
    #    ignore fatal errors and let requests fail, as in pre-1.2 Cassandra
    disk_failure_policy: stop
    
    # Policy for commit disk failures:
    #
    # die
    #   shut down the node and kill the JVM, so the node can be replaced.
    #
    # stop
    #   shut down the node, leaving the node effectively dead, node
    #   can still be inspected via JMX.
    #
    # stop_commit
    #   shutdown the commit log, letting writes collect but
    #   continuing to service reads, as in pre-2.0.5 Cassandra
    #
    # ignore
    #   ignore fatal errors and let the batches fail
    commit_failure_policy: stop
    
    # Maximum size of the native protocol prepared statement cache.
    #
    # Note that specifying a too large value will result in long running GCs and possbily
    # out-of-memory errors. Keep the value at a small fraction of the heap.
    #
    # If you constantly see "prepared statements discarded in the last minute because
    # cache limit reached" messages, the first step is to investigate the root cause
    # of these messages and check whether prepared statements are used correctly -
    # i.e. use bind markers for variable parts.
    #
    # Change the default value only if there are more prepared statements than
    # fit in the cache. In most cases, it is not neccessary to change this value.
    # Constantly re-preparing statements is a performance penalty.
    #
    # Valid value is a number greater than 0. When not set, the default is calculated.
    #
    # The default calculated value is 1/256th of the heap or 10 MB, whichever is greater.
    prepared_statements_cache_size_mb:
    
    # Row cache implementation class name. Available implementations:
    #
    # org.apache.cassandra.cache.OHCProvider
    #   Fully off-heap row cache implementation (default).
    #
    # org.apache.cassandra.cache.SerializingCacheProvider
    #   This is the row cache implementation availabile
    #   in previous releases of Cassandra.
    # row_cache_class_name: org.apache.cassandra.cache.OHCProvider
    
    # Maximum size of the row cache in memory.
    # OHC cache implementation requires additional off-heap memory to manage
    # the map structures and additional in-flight memory during operations before/after cache entries can be
    # accounted against the cache capacity. This overhead is usually small compared to the whole capacity.
    # Do not specify more memory that the system can afford in the worst usual situation and leave some
    # headroom for OS block level cache. Never allow your system to swap.
    #
    # Default value is 0 to disable row caching.
    row_cache_size_in_mb: 0
    
    # Duration in seconds after which the database should save the row cache.
    # Caches are saved to saved_caches_directory as specified in this configuration file.
    #
    # Saved caches greatly improve cold-start speeds, and is relatively cheap in
    # terms of I/O for the key cache. Row cache saving is much more expensive and
    # has limited use.
    #
    # Default is 0 to disable saving the row cache.
    row_cache_save_period: 0
    
    # Number of keys from the row cache to save.
    # Specify 0 (which is the default), meaning all keys are going to be saved
    # row_cache_keys_to_save: 100
    
    # Maximum size of the counter cache in memory.
    #
    # Counter cache helps to reduce counter locks' contention for hot counter cells.
    # In case of RF = 1 a counter cache hit will cause the database to skip the read before
    # write entirely. With RF > 1 a counter cache hit will still help to reduce the duration
    # of the lock hold, helping with hot counter cell updates, but will not allow skipping
    # the read entirely. Only the local (clock, count) tuple of a counter cell is kept
    # in memory, not the whole counter, so it's relatively cheap.
    #
    # NOTE: if you reduce the size, you might not get the hottest keys loaded on startup.
    #
    # When not set, the default value is calculated (min(2.5% of Heap (in MB), 50MB)).
    # Set to 0 to disable counter cache.
    # NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.
    counter_cache_size_in_mb:
    
    # Duration in seconds after which the database should
    # save the counter cache (keys only). Caches are saved to saved_caches_directory as
    # specified in this configuration file.
    #
    # Default is 7200 (2 hours).
    counter_cache_save_period: 7200
    
    # Number of keys from the counter cache to save.
    # Disabled by default. When commented out (disabled), all keys are saved.
    # counter_cache_keys_to_save: 100
    
    # Saved caches directory.
    # If not set, the default directory is $DSE_HOME/data/saved_caches.
    saved_caches_directory: /var/lib/cassandra/saved_caches
    
    # commitlog_sync
    # Valid commitlog_sync values are periodic, group, or batch.
    # 
    # When in batch mode, the database won't ack writes until the commit log
    # has been flushed to disk.  Each incoming write will trigger the flush task.
    # commitlog_sync_batch_window_in_ms is a deprecated value. Previously it had
    # almost no value, and is being removed.
    #
    # commitlog_sync_batch_window_in_ms: 2
    #
    # group mode is similar to batch mode, where the database will not ack writes
    # until the commit log has been flushed to disk. The difference is group
    # mode will wait up to commitlog_sync_group_window_in_ms between flushes.
    #
    # commitlog_sync_group_window_in_ms: 1000
    #
    # The default is periodic. When in periodic mode, writes can be acked immediately
    # and the CommitLog is simply synced every commitlog_sync_period_in_ms.
    commitlog_sync: periodic
    commitlog_sync_period_in_ms: 10000
    
    # The size of the individual commitlog file segments.  A commitlog
    # segment can be archived, deleted, or recycled after all the data
    # in it (potentially from each table in the system) has been
    # flushed to sstables.
    #
    # The default size is 32, which is almost always fine, but if you are
    # archiving commitlog segments (see commitlog_archiving.properties),
    # then you probably want a finer granularity of archiving; 8 or 16 MB
    # is reasonable.
    # Max mutation size is also configurable via max_mutation_size_in_kb setting in
    # cassandra.yaml. When max_mutation_size_in_kb is not set, the calculated default is half the size
    # commitlog_segment_size_in_mb * 1024. This value should be positive and less than 2048.
    #
    # NOTE: If max_mutation_size_in_kb is set explicitly, then commitlog_segment_size_in_mb must
    # be set to at least twice the size of max_mutation_size_in_kb / 1024
    #
    commitlog_segment_size_in_mb: 32
    
    # Compression to apply to the commit log.
    # When not set, the default compression for the commit log is uncompressed.
    # LZ4, Snappy, and Deflate compressors are supported.
    # commitlog_compression:
    #   - class_name: LZ4Compressor
    #     parameters:
    #         -
    
    # Any class that implements the SeedProvider interface and has a
    # constructor that takes a Map<String, String> of parameters is valid.
    seed_provider:
        # Addresses of hosts that are deemed contact points.
        # Database nodes use this list of hosts to find each other and learn
        # the topology of the ring. You _must_ change this if you are running
        # multiple nodes!
        - class_name: org.apache.cassandra.locator.SimpleSeedProvider
          parameters:
              # seeds is actually a comma-delimited list of addresses.
              # Ex: "<ip1>,<ip2>,<ip3>"
              - seeds: "dse-0.dse.default.svc.cluster.local,dse-1.dse.default.svc.cluster.local,dse-2.dse.default.svc.cluster.local"
    
    # Maximum memory to use for SSTable chunk cache and buffer pooling.
    # 32 MB of this memory are reserved for pooling buffers, the rest is used as a
    # cache that holds uncompressed sstable chunks.
    # When not set, the default is calculated as 1/3 of max direct memory (by default, max direct memory
    # is up to total system memory).
    # This pool is allocated off-heap, so is in addition to the memory allocated
    # for heap. The cache also has on-heap overhead which is roughly 128 bytes
    # per chunk (i.e. 0.2% of the reserved size if the default 64k chunk size is
    # used).
    # Memory is allocated only when needed.
    # file_cache_size_in_mb: 4096
    
    # The strategy for optimizing disk read.
    # Possible values are:
    # ssd (for solid state disks, the default). When not set, the default is ssd.
    # spinning (for spinning disks)
    # disk_optimization_strategy: ssd
    
    # Total permitted memory to use for memtables. The database will stop
    # accepting writes when the limit is exceeded until a flush completes,
    # and will trigger a flush based on memtable_cleanup_threshold
    # If omitted, the calculated value for both settings is 1/4 the size of the heap.
    # memtable_heap_space_in_mb: 2048
    # memtable_offheap_space_in_mb: 2048
    
    # Ratio of occupied non-flushing memtable size to total permitted size
    # that will trigger a flush of the largest memtable. Larger mct will
    # mean larger flushes and hence less compaction, but also less concurrent
    # flush activity which can make it difficult to keep your disks fed
    # under heavy write load.
    #
    # memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)
    # memtable_cleanup_threshold: 0.2
    
    # Specify the way the database allocates and manages memtable memory.
    # Options are:
    #
    # heap_buffers
    #   on heap nio buffers
    #
    # offheap_buffers
    #   off heap (direct) nio buffers
    #
    # offheap_objects
    #    off heap objects
    memtable_allocation_type: heap_buffers
    
    # Disk usage threshold that will trigger the database to reclaim some space
    # used by the commit log files.
    #
    # If the commit log disk usage exceeds this threshold, the database will flush
    # every dirty table in the oldest segment and remove it. So a small total
    # commitlog space will cause more flush activity on less-active
    # tables.
    #
    # The default value is the smaller of 8192, and 1/4 of the total space
    # of the commitlog volume.
    #
    # The database will still write commit logs while it reclaims space
    # from previous commit logs. Therefore, the total disk space "reserved"
    # for the commit log should be _at least_ 25% bigger than the value of the
    # commitlog_total_space_in_mb configuration parameter. The actual
    # value depends on the write workload.
    #
    # commitlog_total_space_in_mb: 8192
    
    # The number of memtable flush writer threads per disk and
    # the total number of memtables that can be flushed concurrently.
    # These are generally a combination of compute and IO bound.
    #
    # Memtable flushing is more CPU efficient than memtable ingest and a single thread
    # can keep up with the ingest rate of a whole server on a single fast disk
    # until it temporarily becomes IO bound under contention typically with compaction.
    # At that point you need multiple flush threads. At some point in the future
    # it may become CPU bound all the time.
    #
    # You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation
    # metric, which should be 0. A non-zero metric occurs if threads are blocked waiting on flushing
    # to free memory.
    #
    # memtable_flush_writers defaults to 4 for a single data directory.
    # This means that four memtables can be flushed concurrently to the single data directory.
    # If you have multiple data directories the default is one memtable flushing at a time
    # but the flush will use a thread per data directory so you will get four or more writers.
    #
    # Four is generally enough to flush on a fast disk [array] mounted as a single data directory.
    # Adding more flush writers will result in smaller more frequent flushes that introduce more
    # compaction overhead.
    #
    # There is a direct tradeoff between number of memtables that can be flushed concurrently
    # and flush size and frequency. More is not better you just need enough flush writers
    # to never stall waiting for flushing to free memory.
    #
    # memtable_flush_writers: 4
    
    # Total space to use for change-data-capture logs on disk.
    #
    # If space gets above this value, the database will throw WriteTimeoutException
    # on mutations including CDC-enabled tables. A CDCCompactor is responsible
    # for parsing the raw CDC logs and deleting them when parsing is completed.
    #
    # The default value is calculated as the min of 4096 mb and 1/8th of the total space
    # of the drive where cdc_raw_directory resides.
    # cdc_total_space_in_mb: 4096
    
    # When the cdc_raw limit is reached and the CDCCompactor is running behind
    # or experiencing backpressure, we check at the following interval to see if any
    # new space for cdc-tracked tables has been made available. Default to 250ms
    # cdc_free_space_check_interval_ms: 250
    
    # A fixed memory pool size in MB for for SSTable index summaries. If left
    # empty, this will default to 5% of the heap size. If the memory usage of
    # all index summaries exceeds this limit, SSTables with low read rates will
    # shrink their index summaries in order to meet this limit.  However, this
    # is a best-effort process. In extreme conditions the database may need to use
    # more than this amount of memory.
    index_summary_capacity_in_mb:
    
    # How frequently index summaries should be resampled.  This is done
    # periodically to redistribute memory from the fixed-size pool to sstables
    # proportional their recent read rates.  Setting to -1 will disable this
    # process, leaving existing index summaries at their current sampling level.
    index_summary_resize_interval_in_minutes: 60
    
    # Whether to enable periodic fsync() when doing sequential writing. When enabled, fsync() at intervals
    # force the operating system to flush the dirty
    # buffers. Enable to avoid sudden dirty buffer flushing from
    # impacting read latencies. Almost always a good idea on SSDs; not
    # necessarily on platters.
    trickle_fsync: true
    trickle_fsync_interval_in_kb: 10240
    
    # TCP port, for commands and data.
    # For security reasons, you should not expose this port to the internet.  Firewall it if needed.
    storage_port: 7000
    
    # SSL port, for encrypted communication.  Unused unless enabled in
    # encryption_options
    # For security reasons, you should not expose this port to the internet.  Firewall it if needed.
    ssl_storage_port: 7001
    
    # Address or interface to bind to and tell other nodes to connect to.
    # You _must_ change this address or interface to enable multiple nodes to communicate!
    #
    # Set listen_address OR listen_interface, not both.
    #
    # When not set (blank), InetAddress.getLocalHost() is used. This
    # will always do the Right Thing _if_ the node is properly configured
    # (hostname, name resolution, etc), and the Right Thing is to use the
    # address associated with the hostname (it might not be).
    #
    # Setting listen_address to 0.0.0.0 is always wrong.
    #
    listen_address: localhost
    
    # Set listen_address OR listen_interface, not both. Interfaces must correspond
    # to a single address. IP aliasing is not supported.
    #listen_interface: wlan0
    
    # If you specify the interface by name and the interface has an ipv4 and an ipv6 address,
    # specify which address.
    # If false, the first ipv4 address will be used.
    # If true, the first ipv6 address will be used.
    # When not set, the default is false (ipv4).
    # If there is only one address, that address is selected regardless of ipv4/ipv6.
    # listen_interface_prefer_ipv6: false
    
    # Address to broadcast to other database nodes.
    # Leaving this blank will set it to the same value as listen_address
    # broadcast_address: 1.2.3.4
    
    # When using multiple physical network interfaces, set this
    # to true to listen on broadcast_address in addition to
    # the listen_address, allowing nodes to communicate in both
    # interfaces.
    # Do not set this property if the network configuration automatically
    # routes between the public and private networks such as EC2.
    # listen_on_broadcast_address: false
    
    # Internode authentication backend, implementing IInternodeAuthenticator;
    # used to allow/disallow connections from peer nodes.
    # internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator
    
    # Whether to start the native transport server.
    # The address on which the native transport is bound is defined by native_transport_address.
    start_native_transport: true
    # The port where the CQL native transport listens for clients.
    # For security reasons, do not expose this port to the internet. Firewall it if needed.
    native_transport_port: 9042
    # Enabling native transport encryption in client_encryption_options allows you to use
    # encryption for the standard port or use a dedicated, additional port along with the unencrypted
    # standard native_transport_port.
    # If client encryption is enabled and native_transport_port_ssl is disabled, the
    # native_transport_port (default: 9042) will encrypt all traffic. To use both unencrypted and encrypted
    # traffic, enable native_transport_port_ssl.
    # native_transport_port_ssl: 9142
    #
    # The maximum size of allowed frame. Frame (requests) larger than this will
    # be rejected as invalid. The default is 256 MB. If you're changing this parameter,
    # you may want to adjust max_value_size_in_mb accordingly. This should be positive and less than 2048.
    # native_transport_max_frame_size_in_mb: 256
    
    # The maximum number of concurrent client connections.
    # The default is -1, which means unlimited.
    # native_transport_max_concurrent_connections: -1
    
    # The maximum number of concurrent client connections per source ip.
    # The default is -1, which means unlimited.
    # native_transport_max_concurrent_connections_per_ip: -1
    
    # The address or interface to bind the native transport server to.
    #
    # Set native_transport_address OR native_transport_interface, not both.
    #
    # Leaving native_transport_address blank has the same effect as on listen_address
    # (i.e. it will be based on the configured hostname of the node).
    #
    # Note that unlike listen_address, you can specify 0.0.0.0, but you must also
    # set native_transport_broadcast_address to a value other than 0.0.0.0.
    #
    # For security reasons, you should not expose this port to the internet.  Firewall it if needed.
    native_transport_address: localhost
    
    # Set native_transport_address OR native_transport_interface, not both. Interfaces must correspond
    # to a single address, IP aliasing is not supported.
    # native_transport_interface: eth0
    
    # If you specify the interface by name and the interface has an ipv4 and an ipv6 address,
    # specify which address.
    # If false, the first ipv4 address will be used.
    # If true, the first ipv6 address will be used.
    # When not set, the default is false (ipv4).
    # If there is only one address, that address is selected regardless of ipv4/ipv6.
    # native_transport_interface_prefer_ipv6: false
    
    # Native transport address to broadcast to drivers and other nodes. 
    # Do not set to 0.0.0.0. If left blank, this will be set to the value of
    # native_transport_address. If native_transport_address is set to 0.0.0.0, native_transport_broadcast_address must
    # be set.
    # native_transport_broadcast_address: 1.2.3.4
    
    # enable or disable keepalive on native connections
    native_transport_keepalive: true
    
    # Uncomment to set socket buffer size for internode communication.
    # Note that when setting this, the buffer size is limited by net.core.wmem_max
    # and when not setting it, the buffer size is defined by net.ipv4.tcp_wmem
    # See also:
    # /proc/sys/net/core/wmem_max
    # /proc/sys/net/core/rmem_max
    # /proc/sys/net/ipv4/tcp_wmem
    # /proc/sys/net/ipv4/tcp_wmem
    # and 'man tcp'
    # internode_send_buff_size_in_bytes:
    
    # Uncomment to set socket buffer size for internode communication.
    # Note that when setting this value, the buffer size is limited by net.core.wmem_max
    # and when not setting this value, the buffer size is defined by net.ipv4.tcp_wmem
    # internode_recv_buff_size_in_bytes:
    
    # Whether to create a hard link to each SSTable
    # flushed or streamed locally in a backups/ subdirectory of the
    # keyspace data. Incremental backups enable storing backups off site without transferring entire
    # snapshots. The database does not automatically clear incremental backup files.
    # DataStax recommends setting up a process to clear incremental backup hard links each time a new snapshot is created.
    incremental_backups: false
    
    # Whether to enable snapshots before each compaction.
    # Be careful using this option, since the database won't clean up the
    # snapshots for you. A snapshot is useful to back up data when there is a data format change.
    snapshot_before_compaction: false
    
    # Whether to enable snapshots of the data before truncating a keyspace or
    # dropping a table. To prevent data loss, DataStax strongly advises using the default
    # setting. If you set auto_snapshot to false, you lose data on truncation or drop.
    auto_snapshot: true
    
    # Granularity of the collation index of rows within a partition.
    # Smaller granularity means better search times, especially if
    # the partition is in disk cache, but also higher size of the
    # row index and the associated memory cost for keeping that cached.
    # The performance of lower density nodes may benefit from decreasing
    # this number to 4, 2 or 1kb.
    column_index_size_in_kb: 16
    
    # Threshold for the total size of all index entries for a partition that the database
    # stores in the partition key cache. If the total size of all index entries for a partition
    # exceeds this amount, the database stops putting entries for this partition into the partition
    # key cache.
    #
    # Note that this size refers to the size of the
    # serialized index information and not the size of the partition.
    column_index_cache_size_in_kb: 2
    
    # Number of simultaneous compactions allowed to run simultaneously, NOT including
    # validation "compactions" for anti-entropy repair.  Simultaneous
    # compactions help preserve read performance in a mixed read/write
    # workload by limiting the number of small SSTables that accumulate
    # during a single long running compaction. When not set, the calculated default is usually
    # fine. If you experience problems with compaction running too
    # slowly or too fast, you should first review the
    # compaction_throughput_mb_per_sec option.
    #
    # The calculated default value for concurrent_compactors defaults to the smaller of (number of disks,
    # number of cores), with a minimum of 2 and a maximum of 8.
    #
    # If your data directories are backed by SSD, increase this
    # to the number of cores.
    #concurrent_compactors: 1
    
    # Number of simultaneous repair validations to allow. Default is unbounded
    # Values less than one are interpreted as unbounded (the default)
    # concurrent_validations: 0
    
    # Number of simultaneous materialized view builder tasks to allow.
    concurrent_materialized_view_builders: 2
    
    # Throttles compaction to the specified total throughput across the entire
    # system. The faster you insert data, the faster you need to compact in
    # order to keep the SSTable count down. In general, setting this to
    # 16 to 32 times the rate you are inserting data is more than sufficient.
    # Set to 0 to disable throttling. Note that this throughput applies for all types
    # of compaction, including validation compaction.
    compaction_throughput_mb_per_sec: 16
    
    # The size of the SSTables to trigger preemptive opens. The compaction process opens
    # SSTables before they are completely written and uses them in place
    # of the prior SSTables for any range previously written. This process helps
    # to smoothly transfer reads between the SSTables by reducing page cache churn and keeps hot rows hot.
    #
    # Setting this to a low value will negatively affect performance
    # and eventually cause huge heap pressure and a lot of GC activity.
    # The "optimal" value depends on the hardware and workload.
    #
    # Values <= 0 will disable this feature.
    sstable_preemptive_open_interval_in_mb: 50
    
    # Throttle, in megabits per seconds, for the throughput of all outbound streaming file transfers
    # on a node. The database does mostly sequential I/O when streaming data during
    # bootstrap or repair which can saturate the network connection and degrade
    # client (RPC) performance. When not set, the value is 200 Mbps (25 MB/s).
    # stream_throughput_outbound_megabits_per_sec: 200
    
    # Throttle for all streaming file transfers between the datacenters,
    # this setting allows users to throttle inter dc stream throughput in addition
    # to throttling all network stream traffic as configured with
    # stream_throughput_outbound_megabits_per_sec.
    # When unset, the default is 200 Mbps (25 MB/s).
    # inter_dc_stream_throughput_outbound_megabits_per_sec: 200
    
    # How long the coordinator should wait for read operations to complete.
    # Lowest acceptable value is 10 ms. This timeout does not apply to
    # aggregated queries such as SELECT COUNT(*), MIN(x), etc.
    read_request_timeout_in_ms: 5000
    # How long the coordinator should wait for seq or index scans to complete.
    # Lowest acceptable value is 10 ms. This timeout does not apply to
    # aggregated queries such as SELECT COUNT(*), MIN(x), etc.
    range_request_timeout_in_ms: 10000
    # How long the coordinator should wait for aggregated read operations to complete,
    # such as SELECT COUNT(*), MIN(x), etc.
    aggregated_request_timeout_in_ms: 120000
    # How long the coordinator should wait for writes to complete.
    # Lowest acceptable value is 10 ms.
    write_request_timeout_in_ms: 2000
    # How long the coordinator should wait for counter writes to complete.
    # Lowest acceptable value is 10 ms.
    counter_write_request_timeout_in_ms: 5000
    # How long a coordinator should continue to retry a CAS operation
    # that contends with other proposals for the same row.
    # Lowest acceptable value is 10 ms.
    cas_contention_timeout_in_ms: 1000
    # How long the coordinator should wait for truncates to complete
    # The long default value allows the database to take a snapshot before removing the data.
    # If auto_snapshot is disabled (not recommended), you can reduce this time.
    # Lowest acceptable value is 10 ms.
    truncate_request_timeout_in_ms: 60000
    # The default timeout for other, miscellaneous operations.
    # Lowest acceptable value is 10 ms.
    request_timeout_in_ms: 10000
    # Additional RTT latency between DCs applied to cross dc request. Set this property only when
    # cross dc network latency is high. Value must be non-negative.
    # Set this value to 0 to apply no additional RTT latency. When unset, the default is 0.
    # cross_dc_rtt_in_ms: 0
    
    # How long before a node logs slow queries. SELECT queries that exceed
    # this timeout will generate an aggregated log message to identify slow queries.
    # Set this value to zero to disable slow query logging.
    slow_query_log_timeout_in_ms: 500
    
    # Whether to enable operation timeout information exchange between nodes to accurately
    # measure request timeouts.  If disabled, replicas will assume that requests
    # were forwarded to them instantly by the coordinator. During overload conditions this means extra
    # time is required for processing already-timed-out requests.
    #
    # Warning: Before enabling this property make sure that NTP (network time protocol) is installed
    # and the times are synchronized between the nodes.
    cross_node_timeout: false
    
    # Interval to send keep-alive messages. The stream session fails when a keep-alive message
    # is not received for 2 keep-alive cycles. When unset, the default is 300 seconds (5 minutes)
    # so that a stalled stream times out in 10 minutes (2 cycles).
    # streaming_keep_alive_period_in_secs: 300
    
    # Maximum number of connections per host for streaming.
    # Increase this when you notice that joins are CPU-bound rather that network-
    # bound. For example, a few nodes with large files.
    # streaming_connections_per_host: 1
    
    
    # The sensitivity of the failure detector on an exponential scale. Generally, this setting
    # does not need adjusting. phi value that must be reached for a host to be marked down.
    # When unset, the internal value is 8.
    # phi_convict_threshold: 8
    
    # endpoint_snitch -- A class that implements the IEndpointSnitch interface. The database uses the
    # snitch to locate nodes and route requests. Use only snitch implementations that are bundled with DSE.
    #
    # THE DATABASE WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH
    # AFTER DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.
    # This means that if you start with the default SimpleSnitch, which
    # locates every node on "rack1" in "datacenter1", your only options
    # if you need to add another datacenter are GossipingPropertyFileSnitch
    # (and the older PFS).  From there, if you want to migrate to an
    # incompatible snitch like Ec2Snitch you can do it by adding new nodes
    # under Ec2Snitch (which will locate them in a new "datacenter") and
    # decommissioning the old nodes.
    #
    # Supported snitches from Cassandra:
    #
    # SimpleSnitch:
    #    Treats Strategy order as proximity. This can improve cache
    #    locality when disabling read repair. Appropriate only for
    #    single-datacenter deployments.
    #
    # GossipingPropertyFileSnitch
    #    This should be your go-to snitch for production use.  The rack
    #    and datacenter for the local node are defined in
    #    cassandra-rackdc.properties and propagated to other nodes via
    #    gossip. For migration from the PropertyFileSnitch, uses the cassandra-topology.properties
    #    file if it is present.
    #
    # PropertyFileSnitch:
    #    Proximity is determined by rack and data center, which are
    #    explicitly configured in cassandra-topology.properties.
    #
    # Ec2Snitch:
    #    Appropriate for EC2 deployments in a single Region. Loads Region
    #    and Availability Zone information from the EC2 API. The Region is
    #    treated as the datacenter, and the Availability Zone as the rack.
    #    Only private IPs are used, so this will not work across multiple
    #    Regions.
    #
    # Ec2MultiRegionSnitch:
    #    Uses public IPs as broadcast_address to allow cross-region
    #    connectivity. This means you must also set seed addresses to the public
    #    IP and open the storage_port or
    #    ssl_storage_port on the public IP firewall. For intra-Region
    #    traffic, the database will switch to the private IP after
    #    establishing a connection.
    #
    # RackInferringSnitch:
    #    Proximity is determined by rack and data center, which are
    #    assumed to correspond to the 3rd and 2nd octet of each node's IP
    #    address, respectively.  Unless this happens to match your
    #    deployment conventions, this is best used as an example of
    #    writing a custom Snitch class and is provided in that spirit.
    #
    # DataStax Enterprise (DSE) provides:
    #
    # com.datastax.bdp.snitch.DseSimpleSnitch:
    #    Proximity is determined by DSE workload, which places transactional,
    #    Analytics, and Search nodes into their separate datacenters.
    #    Appropriate only for Development deployments.
    #
    #endpoint_snitch: com.datastax.bdp.snitch.DseSimpleSnitch
    endpoint_snitch: GossipingPropertyFileSnitch
    
    # How often to perform the more expensive part of host score
    # calculation. Use care when reducing this interval, score calculation is CPU intensive.
    dynamic_snitch_update_interval_in_ms: 100
    # How often to reset all host scores, allowing a bad host to
    # possibly recover.
    dynamic_snitch_reset_interval_in_ms: 600000
    # if set greater than zero and read_repair_chance is < 1.0, this will allow
    # 'pinning' of replicas to hosts in order to increase cache capacity.
    # The badness threshold will control how much worse the pinned host has to be
    # before the dynamic snitch will prefer other replicas over it.  This is
    # expressed as a double which represents a percentage.  Thus, a value of
    # 0.2 means the database would continue to prefer the static snitch values
    # until the pinned host was 20% worse than the fastest.
    dynamic_snitch_badness_threshold: 0.1
    
    # Enable or disable inter-node encryption
    # JVM defaults for supported SSL socket protocols and cipher suites can
    # be replaced using custom encryption options. This is not recommended
    # unless you have policies in place that dictate certain settings, or
    # need to disable vulnerable ciphers or protocols in case the JVM cannot
    # be updated.
    # FIPS compliant settings can be configured at JVM level and should not
    # involve changing encryption settings here:
    # https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html
    # *NOTE* No custom encryption options are enabled at the moment
    # The available internode options are : all, none, dc, rack
    #
    # If set to dc, encrypt the traffic between the DCs
    # If set to rack, encrypt the traffic between the racks
    #
    # The passwords used in these options must match the passwords used when generating
    # the keystore and truststore.  For instructions on generating these files, see:
    # http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore
    #
    server_encryption_options:
        internode_encryption: none
        keystore: resources/dse/conf/.keystore
        keystore_password: cassandra
        truststore: resources/dse/conf/.truststore
        truststore_password: cassandra
        # More advanced defaults below:
        # protocol: TLS
        # algorithm: SunX509
        # store_type: JKS
        # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]
        # require_client_auth: false
        # require_endpoint_verification: false
    
    # enable or disable client/server encryption.
    client_encryption_options:
        enabled: false
        # If enabled and optional is set to true, encrypted and unencrypted connections over native transport are handled.
        optional: false
        keystore: resources/dse/conf/.keystore
        keystore_password: cassandra
        # require_client_auth: false
        # Set trustore and truststore_password if require_client_auth is true
        # truststore: resources/dse/conf/.truststore
        # truststore_password: cassandra
        # More advanced defaults below:
        # protocol: TLS
        # algorithm: SunX509
        # store_type: JKS
        # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]
    
    # internode_compression controls whether traffic between nodes is
    # compressed.
    # Can be:
    #
    # all
    #   all traffic is compressed
    #
    # dc
    #   traffic between different datacenters is compressed
    #
    # none
    #   nothing is compressed.
    internode_compression: dc
    
    # Enable or disable tcp_nodelay for inter-dc communication.
    # Disabling it will result in larger (but fewer) network packets being sent,
    # reducing overhead from the TCP protocol itself, at the cost of increasing
    # latency if you block for cross-datacenter responses.
    inter_dc_tcp_nodelay: false
    
    # TTL for different trace types used during logging of the repair process.
    tracetype_query_ttl: 86400
    tracetype_repair_ttl: 604800
    
    # If unset, all GC Pauses greater than gc_log_threshold_in_ms will log at
    # INFO level
    # UDFs (user defined functions) are disabled by default.
    # As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.
    enable_user_defined_functions: false
    
    # Enables scripted UDFs (JavaScript UDFs).
    # Java UDFs are always enabled, if enable_user_defined_functions is true.
    # Enable this option to be able to use UDFs with "language javascript" or any custom JSR-223 provider.
    # This option has no effect, if enable_user_defined_functions is false.
    enable_scripted_user_defined_functions: false
    
    # Optionally disable asynchronous UDF execution.
    # Disabling asynchronous UDF execution also implicitly disables the security-manager!
    # By default, asynchronous UDF execution is enabled to be able to detect UDFs that run too long / forever and be
    # able to fail fast - i.e. stop the Cassandra daemon, which is currently the only appropriate approach to
    # "tell" a user that there's something really wrong with the UDF.
    # When you disable async UDF execution, users MUST pay attention to read-timeouts since these timeouts might indicate
    # UDFs that run too long or forever which can destabilize the cluster.
    # Currently UDFs within the GROUP BY clause are allowed only when asynchronous UDF execution is disabled,
    # subjected to the afforementioned security caveats.
    enable_user_defined_functions_threads: true
    
    # The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.
    # Lowering this value on Windows can provide much tighter latency and better throughput, however
    # some virtualized environments may see a negative performance impact from changing this setting
    # below their system default. The sysinternals 'clockres' tool can confirm your system's default
    # setting.
    windows_timer_interval: 1
    
    
    # Enables encrypting data at-rest (on disk). Different key providers are supported, but the default KSKeyProvider reads from
    # a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by
    # the "key_alias" is the only key that will be used for encrypt opertaions; previously used keys
    # can still (and should!) be in the keystore and will be used on decrypt operations
    # to handle key rotation.
    #
    # DataStax recommends installing Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction
    # Policy Files for your version of the JDK to ensure support of all encryption algorithms.
    # See the DSE installation documentation.
    #
    transparent_data_encryption_options:
        enabled: false
        chunk_length_kb: 64
        cipher: AES/CBC/PKCS5Padding
        key_alias: testing:1
        # CBC IV length for AES must be 16 bytes, the default size
        # iv_length: 16
        key_provider:
          - class_name: org.apache.cassandra.security.JKSKeyProvider
            parameters:
              - keystore: conf/.keystore
                keystore_password: cassandra
                store_type: JCEKS
                key_password: cassandra
    
    
    #####################
    # SAFETY THRESHOLDS #
    #####################
    
    # When executing a scan, within or across a partition, we need to keep the
    # tombstones seen in memory so we can return them to the coordinator, which
    # will use them to make sure other replicas also know about the deleted rows.
    # With workloads that generate a lot of tombstones, this can cause performance
    # problems and even exaust the server heap.
    # (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)
    # Adjust the thresholds here if you understand the dangers and want to
    # scan more tombstones anyway.  These thresholds may also be adjusted at runtime
    # using the StorageService mbean.
    tombstone_warn_threshold: 1000
    tombstone_failure_threshold: 100000
    
    # Log WARN on any multiple-partition batch size that exceeds this value. 64kb per batch by default.
    # Use caution when increasing the size of this threshold as it can lead to node instability.
    batch_size_warn_threshold_in_kb: 64
    
    # Fail any multiple-partition batch that exceeds this value. The calcuated default is 640kb (10x warn threshold).
    batch_size_fail_threshold_in_kb: 640
    
    # Log WARN on any batches not of type LOGGED than span across more partitions than this limit.
    unlogged_batch_across_partitions_warn_threshold: 10
    
    # Log a warning when compacting partitions larger than this value.
    compaction_large_partition_warning_threshold_mb: 100
    
    # GC Pauses greater than 200 ms will be logged at INFO level.
    # Adjust this threshold to minimize logging, if necessary.
    # gc_log_threshold_in_ms: 200
    
    # GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level.
    # Adjust this threshold based on your application throughput requirement.
    # Set to 0 to deactivate the feature.
    # gc_warn_threshold_in_ms: 1000
    
    # Maximum size of any value in SSTables. Safety measure to detect SSTable corruption
    # early. Any value size larger than this threshold will result in marking an SSTable
    # as corrupted. This value should be positive and less than 2048.
    # max_value_size_in_mb: 256
    
    # Probability the database will gossip with one of the seed nodes during each round of gossip.
    # Valid range is between 0.01 and 1.0
    # seed_gossip_probability: 1.0
    
    # Back-pressure settings #
    # If enabled, the coordinator will apply the back-pressure strategy specified below to each mutation
    # sent to replicas, with the aim of reducing pressure on overloaded replicas.
    back_pressure_enabled: false
    # The back-pressure strategy applied.
    # The default implementation, RateBasedBackPressure, takes three arguments:
    # high ratio, factor, and flow type, and uses the ratio between incoming mutation responses and outgoing mutation requests.
    # If below high ratio, outgoing mutations are rate limited according to the incoming rate decreased by the given factor;
    # if above high ratio, the rate limiting is increased by the given factor;
    # the recommended factor is a whole number between 1 and 10, use larger values for a faster recovery
    # at the expense of potentially more dropped mutations;
    # the rate limiting is applied according to the flow type: if FAST, it's rate limited at the speed of the fastest replica,
    # if SLOW at the speed of the slowest one.
    # New strategies can be added. Implementors need to implement org.apache.cassandra.net.BackpressureStrategy and
    # provide a public constructor that accepts Map<String, Object>.
    back_pressure_strategy:
        - class_name: org.apache.cassandra.net.RateBasedBackPressure
          parameters:
            - high_ratio: 0.90
              factor: 5
              flow: FAST
    
    # Coalescing Strategies #
    # Coalescing multiples messages turns out to significantly boost message processing throughput (think doubling or more).
    # On bare metal, the floor for packet processing throughput is high enough that many applications won't notice, but in
    # virtualized environments, the point at which an application can be bound by network packet processing can be
    # surprisingly low compared to the throughput of task processing that is possible inside a VM. It's not that bare metal
    # doesn't benefit from coalescing messages, it's that the number of packets a bare metal network interface can process
    # is sufficient for many applications such that no load starvation is experienced even without coalescing.
    # There are other benefits to coalescing network messages that are harder to isolate with a simple metric like messages
    # per second. By coalescing multiple tasks together, a network thread can process multiple messages for the cost of one
    # trip to read from a socket, and all the task submission work can be done at the same time reducing context switching
    # and increasing cache friendliness of network message processing.
    # See CASSANDRA-8692 for details.
    
    # Strategy to use for coalescing messages in OutboundTcpConnection.
    # Can be fixed, movingaverage, timehorizon, disabled (default).
    # You can also specify a subclass of CoalescingStrategies.CoalescingStrategy by name. Disabled by default.
    # otc_coalescing_strategy: DISABLED
    
    # How many microseconds to wait for coalescing. For fixed strategy, this is the amount of time after the first
    # message is received before it will be sent with any accompanying messages. For movingaverage strategy, this is the
    # maximum amount of time that will be waited as well as the interval at which messages must arrive on average
    # for coalescing to be enabled.
    # otc_coalescing_window_us: 200
    
    # Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.
    # otc_coalescing_enough_coalesced_messages: 8
    
    # Continuous paging settings. When requested by the client, pages are pushed continuously to the client.
    # These settings are used to calculate the maximum memory used:
    # (max_concurrent_sessions * max_session_pages * max_page_size_mb).
    # The default values (60 x 4 x 8) = 1920 MB of maximum memory used. The only case in which a page may be bigger than
    # max_page_size_mb is if an individual CQL row is larger than this value.
    continuous_paging:
        # The maximum number of concurrent sessions, any additional session will be rejected with an unavailable error.
        max_concurrent_sessions: 60
        # The maximum number of pages that can be buffered for each session
        max_session_pages: 4
        # The maximum size of a page, in MB. If an individual CQL row is larger than this value, the page can be larger than
        # this value.
        max_page_size_mb: 8
        # The maximum time in milliseconds for which a local continuous query will run, assuming the client continues
        # reading or requesting pages. When this threshold is exceeded, the session is swapped out and rescheduled.
        # Swapping and rescheduling resources ensures the release of resources including those that prevent the memtables
        # from flushing. Adjust when high write workloads exist on tables that have
        # continuous paging requests.
        max_local_query_time_ms: 5000
        # The maximum time the server will wait for a client to request more pages, in seconds, assuming the
        # server queue is full or the client has not required any more pages via a backpressure update request.
        # Increase this value for extremely large page sizes (max_page_size_mb)
        # or for extremely slow networks.
        client_timeout_sec: 600
        # How long the server waits for a cancel request to complete, in seconds.
        cancel_timeout_sec: 5
        # How long the server will wait, in milliseconds, before checking if a continuous paging session can be resumed when
        # the session is paused because of backpressure.
        paused_check_interval_ms: 1
    
    # Track a metric per keyspace indicating whether replication achieved the ideal consistency
    # level for writes without timing out. This is different from the consistency level requested by
    # each write which may be lower in order to facilitate availability.
    # ideal_consistency_level: EACH_QUORUM
    
    # NodeSync settings.
    nodesync:
        # The (maximum) rate (in kilobytes per second) for data validation.
        rate_in_kb: 1024
    
    # TPC settings - WARNING it is generally not advised to change these values unless directed by a performance expert
    
    # Number of cores used by the internal Threads Per Core architecture (TPC). This setting corresponds to the
    # number of event loops that will be created internally. Do not tune. DataStax recommends contacting the DataStax Services
    # team before changing this value. If unset or commented out (the default), the calculated default value is the number
    # of available processors on the machine minus one.
    # tpc_cores:
    
    # Number of cores used for reads. Do not tune. DataStax recommends contacting the DataStax Services
    # team before changing this value. By default this is set to min(tpc_cores, io_global_queue_depth / 4), which means
    # that each IO queue must have at least a local depth of 4 and we choose a number of IO queues, or IO cores, such that the
    # combined depth does not exceed io_global_queue_depth, capped to the number of TPC cores.
    # tpc_io_cores:
    
    # The global IO queue depth that is used for reads when AIO is enabled (the default for SSDs).
    # The default value used is the value in /sys/class/block/sd[a|b...]/queue/nr_requests,
    # which is typically 128. This default value is a starting point for tuning. You can also run tools/bin/disk_cal.py to
    # determine the ideal queue depth for a specific disk. However, capping to the ideal
    # queue depth assumes that all TPC IO cores will be fully working during read workloads. If that's not the case,
    # you might want to double the ideal queue depth, for example. Exceeding the value used by the Linux IO scheduler (128)
    # is never advantageous and will result in higher latency.
    # Do _not_ tune. DataStax recommends contacting the DataStax Services team before changing this value.
    # io_global_queue_depth:
    
  
  jvm.options: |
    ###########################################################################
    #                             jvm.options                                 #
    #                                                                         #
    # - all flags defined here will be used by cassandra to startup the JVM   #
    # - one flag should be specified per line                                 #
    # - lines that do not start with '-' will be ignored                      #
    # - only static flags are accepted (no variables or parameters)           #
    # - dynamic flags will be appended to these on cassandra-env              #
    ###########################################################################
    
    ######################
    # STARTUP PARAMETERS #
    ######################
    
    # Uncomment any of the following properties to enable specific startup parameters
    
    # In a multi-instance deployment, multiple Cassandra instances will independently assume that all
    # CPU processors are available to it. This setting allows you to specify a smaller set of processors
    # and perhaps have affinity.
    #-Dcassandra.available_processors=number_of_processors
    
    # The directory location of the cassandra.yaml file.
    #-Dcassandra.config=directory
    
    # Sets the initial partitioner token for a node the first time the node is started.
    #-Dcassandra.initial_token=token
    
    # Set to false to start Cassandra on a node but not have the node join the cluster.
    #-Dcassandra.join_ring=true|false
    
    # Set to false to clear all gossip state for the node on restart. Use when you have changed node
    # information in cassandra.yaml (such as listen_address).
    #-Dcassandra.load_ring_state=true|false
    
    # Enable pluggable metrics reporter. See Pluggable metrics reporting in Cassandra 2.0.2.
    #-Dcassandra.metricsReporterConfigFile=file
    
    # Set the port on which the CQL native transport listens for clients. (Default: 9042)
    #-Dcassandra.native_transport_port=port
    
    # Overrides the partitioner. (Default: org.apache.cassandra.dht.Murmur3Partitioner)
    #-Dcassandra.partitioner=partitioner
    
    # To replace a node that has died, restart a new node in its place specifying the address of the
    # dead node. The new node must not have any data in its data directory, that is, it must be in the
    # same state as before bootstrapping.
    #-Dcassandra.replace_address=listen_address or broadcast_address of dead node
    
    # Allow restoring specific tables from an archived commit log.
    #-Dcassandra.replayList=table
    
    # Allows overriding of the default RING_DELAY (30000ms), which is the amount of time a node waits
    # before joining the ring.
    #-Dcassandra.ring_delay_ms=ms
    
    # Set the SSL port for encrypted communication. (Default: 7001)
    #-Dcassandra.ssl_storage_port=port
    
    # Set the port for inter-node communication. (Default: 7000)
    #-Dcassandra.storage_port=port
    
    # Set the default location for the trigger JARs. (Default: conf/triggers)
    #-Dcassandra.triggers_dir=directory
    
    # For testing new compaction and compression strategies. It allows you to experiment with different
    # strategies and benchmark write performance differences without affecting the production workload. 
    #-Dcassandra.write_survey=true
    
    # To disable configuration via JMX of auth caches (such as those for credentials, permissions and
    # roles). This will mean those config options can only be set (persistently) in cassandra.yaml
    # and will require a restart for new values to take effect.
    #-Dcassandra.disable_auth_caches_remote_configuration=true
    
    # To disable dynamic calculation of the page size used when indexing an entire partition (during
    # initial index build/rebuild). If set to true, the page size will be fixed to the default of
    # 10000 rows per page.
    #-Dcassandra.force_default_indexing_page_size=true
    
    # Imposes an upper bound on hint lifetime below the normal min gc_grace_seconds
    #-Dcassandra.maxHintTTL=max_hint_ttl_in_seconds
    
    ########################
    # GENERAL JVM SETTINGS #
    ########################
    
    # enable assertions. highly suggested for correct application functionality.
    -ea
    
    # enable thread priorities, primarily so we can give periodic tasks
    # a lower priority to avoid interfering with client workload
    -XX:+UseThreadPriorities
    
    # allows lowering thread priority without being root on linux - probably
    # not necessary on Windows but doesn't harm anything.
    # see http://tech.stolsvik.com/2010/01/linux-java-thread-priorities-workaround.html
    -XX:ThreadPriorityPolicy=42
    
    # Enable heap-dump if there's an OOM
    -XX:+HeapDumpOnOutOfMemoryError
    
    # Per-thread stack size.
    -Xss256k
    
    # Larger interned string table, for gossip's benefit (CASSANDRA-6410)
    -XX:StringTableSize=1000003
    
    # Make sure all memory is faulted and zeroed on startup.
    # This helps prevent soft faults in containers and makes
    # transparent hugepage allocation more effective.
    -XX:+AlwaysPreTouch
    
    # Enable thread-local allocation blocks and allow the JVM to automatically
    # resize them at runtime.
    -XX:+UseTLAB
    -XX:+ResizeTLAB
    -XX:+UseNUMA
    
    # http://www.evanjones.ca/jvm-mmap-pause.html
    -XX:+PerfDisableSharedMem
    
    # Prefer binding to IPv4 network intefaces (when net.ipv6.bindv6only=1). See
    # http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6342561 (short version:
    # comment out this entry to enable IPv6 support).
    -Djava.net.preferIPv4Stack=true
    
    # Align direct memory allocations
    -Dsun.nio.PageAlignDirectMemory=true
    
    # Honor user code @Contended annotations
    -XX:-RestrictContended
    
    # Decrease frequency of guaranteed safepoints to avoid needless pauses
    -XX:+UnlockDiagnosticVMOptions
    -XX:GuaranteedSafepointInterval=300000
    
    # Disable biased locking to avoid biased lock revokation pauses
    -XX:-UseBiasedLocking
    
    ### Debug/profiling options
    
    # There's no adverse side effect to enabling the debug info, so leave on by default to allow better info for profilers
    -XX:+UnlockDiagnosticVMOptions
    -XX:+DebugNonSafepoints
    
    # If you plan to profile using perf you will need to enable this option
    #-XX:+PreserveFramePointer
    
    # Uncomment to enable flight recorder (Use in production is subject to Oracle licensing)
    #-XX:+UnlockCommercialFeatures
    #-XX:+FlightRecorder
    
    
    # uncomment to have Cassandra JVM listen for remote debuggers/profilers on port 1414
    #-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=1414
    
    # uncomment to have Cassandra JVM log internal method compilation (developers only)
    #-XX:+LogCompilation
    
    #################
    # HEAP SETTINGS #
    #################
    
    # Heap size is automatically calculated by cassandra-env based on this
    # formula: max(min(1/2 ram, 1024MB), min(1/4 ram, 8GB))
    # That is:
    # - calculate 1/2 ram and cap to 1024MB
    # - calculate 1/4 ram and cap to 8192MB
    # - pick the max
    #
    # For production use you may wish to adjust this for your environment.
    # If that's the case, uncomment the -Xmx and Xms options below to override the
    # automatic calculation of JVM heap memory.
    #
    # It is recommended to set min (-Xms) and max (-Xmx) heap sizes to
    # the same value to avoid stop-the-world GC pauses during resize, and
    # so that we can lock the heap in memory on startup to prevent any
    # of it from being swapped out.
    #-Xms4G
    #-Xmx4G
    
    # Young generation size is automatically calculated by cassandra-env
    # based on this formula: min(100 * num_cores, 1/4 * heap size)
    #
    # The main trade-off for the young generation is that the larger it
    # is, the longer GC pause times will be. The shorter it is, the more
    # expensive GC will be (usually).
    #
    # It is not recommended to set the young generation size if using the
    # G1 GC, since that will override the target pause-time goal.
    # More info: http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html
    #
    # The example below assumes a modern 8-core+ machine for decent
    # times. If in doubt, and if you do not particularly want to tweak, go
    # 100 MB per physical CPU core.
    #-Xmn800M
    
    ################
    # NIO SETTINGS #
    ################
    
    # Limit maximum cached thread local direct buffer to 1MB to avoid native OOM
    # See https://bugs.openjdk.java.net/browse/JDK-8147468
    -Djdk.nio.maxCachedBufferSize=1048576
    
    ###################################
    # EXPIRATION DATE OVERFLOW POLICY #
    ###################################
    
    # Defines how to handle INSERT requests with TTL exceeding the maximum supported expiration date:
    # * REJECT: this is the default policy and will reject any requests with expiration date timestamp after 2038-01-19T03:14:06+00:00.
    # * CAP: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on 2038-01-19T03:14:06+00:00 and the client will receive a warning.
    # * CAP_NOWARN: same as previous, except that the client warning will not be emitted.
    #
    #-Dcassandra.expiration_date_overflow_policy=REJECT
    
    #################
    #  GC SETTINGS  #
    #################
    
    ### CMS Settings (comment out the G1 section and uncomment section below to enable)
    #-XX:+UseParNewGC
    #-XX:+UseConcMarkSweepGC
    #-XX:+CMSParallelRemarkEnabled
    #-XX:SurvivorRatio=8
    #-XX:MaxTenuringThreshold=1
    #-XX:CMSInitiatingOccupancyFraction=75
    #-XX:+UseCMSInitiatingOccupancyOnly
    #-XX:CMSWaitDuration=10000
    #-XX:+CMSParallelInitialMarkEnabled
    #-XX:+CMSEdenChunksRecordAlways
    ## some JVMs will fill up their heap when accessed via JMX, see CASSANDRA-6541
    #-XX:+CMSClassUnloadingEnabled
    
    ### G1 Settings
    
    # Use the Hotspot garbage-first collector.
    -XX:+UseG1GC
    -XX:+ParallelRefProcEnabled
    
    # Have the JVM do less remembered set work during STW, instead
    # preferring concurrent GC. Reduces p99.9 latency.
    -XX:G1RSetUpdatingPauseTimePercent=5
    
    # Main G1GC tunable: lowering the pause target will lower throughput and vise versa.
    # 200ms is the JVM default and lowest viable setting
    # 1000ms increases throughput. Keep it smaller than the timeouts in cassandra.yaml.
    -XX:MaxGCPauseMillis=500
    
    ## Optional G1 Settings
    
    # Save CPU time on large (>= 16GB) heaps by delaying region scanning
    # until the heap is 70% full. The default in Hotspot 8u40 is 40%.
    #-XX:InitiatingHeapOccupancyPercent=70
    
    # For systems with > 8 cores, the default ParallelGCThreads is 5/8 the number of logical cores.
    # Otherwise equal to the number of cores when 8 or less.
    # Machines with > 10 cores should try setting these to <= full cores.
    #-XX:ParallelGCThreads=16
    # By default, ConcGCThreads is 1/4 of ParallelGCThreads.
    # Setting both to the same value can reduce STW durations.
    #-XX:ConcGCThreads=16
    
    ### GC logging options -- uncomment to enable
    
    -XX:+PrintGCDetails
    -XX:+PrintGCDateStamps
    -XX:+PrintHeapAtGC
    -XX:+PrintTenuringDistribution
    -XX:+PrintGCApplicationStoppedTime
    -XX:+PrintPromotionFailure
    #-XX:PrintFLSStatistics=1
    # A default location is set in cassandra-env.sh
    #-Xloggc:/var/log/cassandra/gc.log
    -XX:+UseGCLogFileRotation
    -XX:NumberOfGCLogFiles=10
    -XX:GCLogFileSize=10M
    
    # Netty queue limit (per core). Effectively acts as an amount of "pending" requests scheduled by Netty
    # and tasks scheduled from TPC tasks.
    -Dio.netty.eventLoop.maxPendingTasks=65536
    
    # The newline in the end of file is intentional
    
  
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dse-dse-server-dse-conf-configmap
  namespace: "dev"
  labels:
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
data:

  
  dse.yaml: |
    # Memory limit for DSE In-Memory tables as a fraction of system memory. When not set,
    # the default is 0.2 (20% of system memory).
    # Specify max_memory_to_lock_fraction or max_memory_to_lock_mb, not both.
    
    # max_memory_to_lock_fraction: 0.20
    
    # Memory limit for DSE In-Memory tables as a maximum in MB.  When not set,
    # max_memory_to_lock_fraction is used.  The max_memory_to_lock_fraction
    # value is ignored if max_memory_to_lock_mb is set to a non-zero value.
    # Specify max_memory_to_lock_fraction or max_memory_to_lock_mb, not both.
    
    # max_memory_to_lock_mb: 10240
    
    ##########################
    # Authentication options
    #
    # These options are used if the authenticator option in cassandra.yaml is set to
    # com.datastax.bdp.cassandra.auth.DseAuthenticator
    #
    # The enabled option controls whether the DseAuthenticator will authenticate users. If
    # set to true users will be authenticated, if set to false they will not.
    # When not set enabled is false.
    #
    # DseAuthenticator allows multiple authentication schemes to be used at the same time.
    # The schemes to be used are controlled by the default_scheme and other_schemes options.
    # A driver can select the scheme to use during authentication.
    #
    # The default_scheme option selects which authentication scheme will be used if the driver
    # does not request a specific scheme. This can be one of the following values:
    #   internal - plain text authentication using the internal password authenticator
    #   ldap     - plain text authentication using the passthrough LDAP authenticator
    #   kerberos - GSSAPI authentication using the Kerberos authenticator
    # The other_schemes option is a list of schemes that can also be selected for use by a
    # driver and can be a list of the above schemes.
    #
    # The scheme_permissions option controls whether roles need to have permission granted to
    # them in order to use specific authentication schemes. These permissions can be granted
    # only when the DseAuthorizer is used.
    #
    # The allow_digest_with_kerberos option controls whether Digest-MD5 authentication is also
    # allowed when Kerberos is one of the authentication schemes. If set to false, it will not
    # be allowed. You must set allow_digest_with_kerberos to true in analytics clusters to use Hadoop
    # inter-node authentication with Hadoop and Spark jobs.
    #
    # The plain_text_without_ssl controls how the DseAuthenticator reacts to plain text
    # authentication requests over unencrypted client connections. It can be one of:
    #   block  - block the request with an authentication error
    #   warn   - log a warning about the request but allow it to continue
    #   allow  - allow the request without any warning
    #
    # The transitional_mode option allows the DseAuthenticator to operate in a transitional
    # mode during setup of authentication in a cluster. This can be one of the following values:
    #   disabled   - transitional mode is disabled
    #   permissive - Only super users are authenticated and logged in, all other
    #                authentication attempts will be logged in as the anonymous user
    #   normal     - If credentials are passed they are authenticated. If the
    #                authentication is successful then the user is logged in, otherwise
    #                the user is logged in as anonymous. If no credentials are passed,
    #                then the user is logged in as anonymous
    #   strict     - If credentials are passed they are authenticated. If the
    #                authentication is successful, the user is logged in. If the
    #                authentication fails, an authentication error is returned. If no
    #                credentials are passed, the user is logged in as anonymous
    # authentication_options:
    #     enabled: false
    #     default_scheme: internal
    #     other_schemes:
    #     scheme_permissions: false
    #     allow_digest_with_kerberos: true
    #     plain_text_without_ssl: warn
    #     transitional_mode: disabled
    
    ##########################
    # Role management options
    #
    # These options are used when the role_manager option in cassandra.yaml is set to
    # com.datastax.bdp.cassandra.auth.DseRoleManager
    #
    # mode can be one of:
    #   internal - the granting and revoking of roles is managed internally
    #              using the GRANT ROLE and REVOKE ROLE statements
    #   ldap - the granting and revoking of roles is managed by an external
    #          LDAP server configured using the ldap_options.
    # role_management_options:
    #     mode: internal
    
    ##########################
    # Authorization options
    #
    # These options are used if the authorization option in cassandra.yaml is set to
    # com.datastax.bdp.cassandra.auth.DseAuthorizer
    #
    # The enabled option controls whether the DseAuthorizer will perform authorization. If
    # set to true authorization is performed, if set to false it is not.
    # When not set, enabled is false.
    #
    # The transitional_mode option allows the DseAuthorizer to operate in a transitional
    # mode during setup of authorization in a cluster. This can be one of the following values:
    #   disabled   - transitional mode is disabled, all connections must provide valid credentials and
    #                map to a login-enabled role
    #   normal     - allow all connections that provide credentials, permissions can be granted to
    #                resources but are not enforced
    #   strict     - permissions can be granted to resources and are enforced on
    #                authenticated users. They are not enforced against anonymous
    #                users
    #
    # allow_row_level_security - To use row level security, set to true for the entire system.
    #                            Use the same setting on all nodes.
    # authorization_options:
    #     enabled: false
    #     transitional_mode: disabled
    #     allow_row_level_security: false
    
    ##########################
    # Kerberos options
    #
    # keytab is <path_to_keytab>/dse.keytab
    # The keytab file must contain the credentials for both of the fully resolved principal names, which
    # replace _HOST with the fully qualified domain name (FQDN) of the host in the service_principal and
    # http_principal settings. The UNIX user running DSE must also have read permissions on the keytab.
    #
    # The service_principal is the DataStax Enterprise process runs under must use the form
    # <dse_user>/_HOST@<REALM>
    #
    # The http_principal is used by the Tomcat application container to run DSE Search.
    #
    # The qop is the Quality of Protection (QOP) values that clients and servers
    # can use for each connection.  Valid values are:
    #   auth      - (default) authentication only
    #   auth-int  - authentication plus integity protection of all transmitted data
    #   auth-conf - authentication plus integrity protection and encryption of all
    #               transmitted data
    #
    # Warning - Encryption using auth-conf is separate and completely independent
    #           of whether encryption is done using SSL.  If auth-conf is selected here
    #           and SSL is enabled, the transmitted data is encrypted twice.
    kerberos_options:
        keytab: resources/dse/conf/dse.keytab
        service_principal: dse/_HOST@REALM
        http_principal: HTTP/_HOST@REALM
        qop: auth
    
    ##########################
    # LDAP options
    #
    # These are options are used when the com.datastax.bdp.cassandra.auth.LdapAuthenticator
    # is configured as the authenticator in cassandra.yaml. When not set, LDAP authentication is not used.
    
    # ldap_options:
    #     The host name of the LDAP server. LDAP on the same host (localhost) is appropriate only in
    #     single node test or development environments.
    #     server_host:
    #
    #     # The port on which the LDAP server listens, usually port 389 for unencrypted
    #     # connections and port 636 for SSL-encrypted connections. If use_tls is set to true, use the
    #     # unencrypted port
    #     server_port: 389
    #
    #     # The distinguished name (DN) of an account that is used to search for other users on the
    #     # LDAP server. This user should have only the necessary permissions to do the search
    #     # If not present then an anonymous bind is used for the search
    #     search_dn:
    #
    #     # Password of the search_dn account
    #     search_password:
    #
    #     # Set to true to use an SSL encrypted connection. In this case the server_port needs
    #     # to be set to the LDAP port for the server
    #     use_ssl: false
    #
    #     # Set to true to initiate a TLS encrypted connection on the default ldap port
    #     use_tls: false
    #
    #     truststore_path:
    #     truststore_password:
    #     truststore_type: jks
    #     user_search_base:
    #     user_search_filter: (uid={0})
    #
    #     # Set to the attribute on the user entry containing group membership information.
    #     user_memberof_attribute: memberof
    #
    #     # The group_search_type defines how group membership will be determined for a user. It
    #     # can be one of:
    #     #     directory_search - will do a subtree search of group_search_base using
    #     #                        group_search_filter to filter the results
    #     #     memberof_search  - will get groups from the memberof attribute of the user. This
    #     #                        requires the directory server to have memberof support
    #     group_search_type: directory_search
    #     group_search_base:
    #     group_search_filter: (uniquemember={0})
    #
    #     # The attribute in the group entry that holds the group name.
    #     group_name_attribute: cn
    #
    #     # Validity period for the credentials cache in milli-seconds (remote bind is an expensive
    #     # operation). Defaults to 0, set to 0 to disable.
    #     credentials_validity_in_ms: 0
    #
    #     # Validity period for the search cache in seconds. Defaults to 0, set to 0 to disable.
    #     search_validity_in_seconds: 0
    #
    #     connection_pool:
    #         max_active: 8
    #         max_idle: 8
    
    # To ensure that records with TTLs are purged from DSE Search indexes when they expire, DSE
    # periodically checks all indexes for expired documents and deletes them. These settings
    # control the scheduling and execution of those checks.
    ttl_index_rebuild_options:
    
        # By default, schedule a check every 300 seconds:
        fixed_rate_period: 300
    
        # The number of seconds to delay the first check to speed up startup time:
        initial_delay: 20
    
        # All documents determined to be expired are deleted from the index during each check, but
        # to avoid memory pressure, their unique keys are retrieved and deletes issued in batches.
        # This determines the maximum number of documents per batch:
        max_docs_per_batch: 4096
    
        # Maximum number of search indexes that can execute TTL cleanup concurrently:
        thread_pool_size: 1
    
    # DSE Search resource upload size limit in MB. A value of '0' disables resource uploading.
    solr_resource_upload_limit_mb: 10
    
    # Transport options for inter-node communication between DSE Search nodes.
    shard_transport_options:
        # The cumulative shard request timeout, in milliseconds, defines the internal timeout for all
        # search queries to prevent long running queries. Default is 60000 (1 minute).
        netty_client_request_timeout: 60000
    
    # ---- DSE Search index encryption options
    
    # solr_encryption_options:
    #     # Whether to allocate shared index decryption cache off JVM heap.
    #     # Default is off heap allocation (true).
    #     decryption_cache_offheap_allocation: true
    
    #     # The maximum size of shared DSE Search decryption cache, in MB.
    #     # Default is 256 MB.
    #     decryption_cache_size_in_mb: 256
    
    # ---- DSE Search indexing settings
    
    # # The maximum number of queued partitions during search index rebuilding. (This serves primarily
    # # as a safeguard against excessive heap usage by the indexing queue.) If set lower than the
    # # number of TPC threads, not all TPC threads can be actively indexing.
    # #
    # # Default: 1024
    # back_pressure_threshold_per_core: 1024
    #
    # # The max time to wait for flushing of index updates during re-index.
    # # Flushing should always complete successfully, in order to fully sync search indexes
    # # with DSE data. DataStax recommends to always set at a reasonably high value.
    # #
    # # Default: 5 minutes
    # flush_max_time_per_core: 5
    #
    # # The maximum time to wait for each search index to load on startup and create/reload search index operations.
    # # Only change this advanced option if any exceptions happen during search index loading.
    # #
    # # Default: 5 minutes
    # load_max_time_per_core: 5
    #
    # # Applies the configured Cassandra disk failure policy to index write failures.
    # # Default is disabled (false).
    # enable_index_disk_failure_policy: false
    
    # # The directory to store search index data. Each DSE Search index is stored under
    # # a solrconfig_data_dir/keyspace.table directory.
    # # Default is a solr.data directory inside Cassandra data directory, or as specified
    # # by the dse.solr.data.dir system property.
    # solr_data_dir: /MyDir
    
    # # The Lucene field cache has been deprecated. Instead set docValues="true" on the field
    # # in the schema.xml file.  After changing the schema, reload and reindex the search index.
    # # Default: false
    # solr_field_cache_enabled: false
    
    # # Global Lucene RAM buffer usage thresholds (separate for heap and off-heap) at which DSE will force segment flush.
    # # Setting this too low may induce a state of constant flushing during periods of ongoing write activity. For
    # # NRT, these forced segment flushes will also de-schedule pending auto-soft commits to avoid potentially
    # # flushing too many small segments.
    # # Default: 1024
    # ram_buffer_heap_space_in_mb: 1024
    # # Default: 1024
    # ram_buffer_offheap_space_in_mb: 1024
    
    # ---- DSE Search CQL query options
    
    # # Maximum time in milliseconds to wait for all rows
    # # to be read from the database during CQL Solr queries.
    # # Default is 10000 (10 seconds).
    # cql_solr_query_row_timeout: 10000
    
    ##########################
    # Global performance service options
    
    # # Number of background threads used by the performance service under normal conditions.
    # # Defaults to 4.
    # performance_core_threads: 4
    # # Maximum number of background threads used by the performance service.
    # # Defaults to concurrent_writes specified in cassandra.yaml.
    # performance_max_threads: 32
    #
    # # The number of queued tasks in the backlog when the number of performance_max_threads are busy (minimum 0).
    # performance_queue_capacity: 32000
    #
    # # If the performance service requests more tasks than (performance_max_threads + performance_queue_capacity),
    # # a dropped task warning will be issued. This warning indicates that collected statistics may not be up to date
    # # because the server couldn't keep up under the current load.
    #
    # # You can disable some services, reconfigure some services, or increase the queue size.
    
    ##########################
    # Performance service options
    
    graph_events:
        ttl_seconds: 600
    
    # cql_slow_log_options:
    #     enabled: true
    #
    #     #  When t > 1, log queries taking longer than t milliseconds.
    #     #      0 <= t <= 1,  log queries above t percentile
    #     threshold: 200.0
    #
    #     # Initial number of queries before percentile filter becomes active
    #     minimum_samples: 100
    #
    #     ttl_seconds: 259200
    #
    #     # Keeps slow queries in-memory only and doesn't write data to the database.
    #     # WARNING - if this is set to 'false' then set threshold >= 2000, otherwise there will be a
    #     # high load on the database.
    #     skip_writing_to_db: true
    #
    #     # The number of slow queries to keep in-memory
    #     num_slowest_queries: 5
    
    cql_system_info_options:
        enabled: false
        refresh_rate_ms: 10000
    
    resource_level_latency_tracking_options:
        enabled: false
        refresh_rate_ms: 10000
    
    db_summary_stats_options:
        enabled: false
        refresh_rate_ms: 10000
    
    cluster_summary_stats_options:
        enabled: false
        refresh_rate_ms: 10000
    
    spark_cluster_info_options:
        enabled: false
        refresh_rate_ms: 10000
    
    # ---- Spark application stats options
    spark_application_info_options:
        enabled: false
        refresh_rate_ms: 10000
    
        driver:
            # enables or disables writing of the metrics collected at Spark Driver to Cassandra
            sink: false
    
            # enables or disables Spark Cassandra Connector metrics at Spark Driver
            connectorSource: false
    
            # enables or disables JVM heap and GC metrics at Spark Driver
            jvmSource: false
    
            # enables or disables application state metrics
            stateSource: false
    
        executor:
            # enables or disables writing of the metrics collected at executors to Cassandra
            sink: false
    
            # enables or disables Spark Cassandra Connector metrics at executors
            connectorSource: false
    
            # enables or disables JVM heap and GC metrics at executors
            jvmSource: false
    
    # Table Histogram data tables options
    histogram_data_options:
        enabled: false
        refresh_rate_ms: 10000
        retention_count: 3
    
    # User/Resource latency tracking settings
    user_level_latency_tracking_options:
        enabled: false
        refresh_rate_ms: 10000
        top_stats_limit: 100
        quantiles: false
    
    # ---- DSE Search Performance Objects
    
    solr_slow_sub_query_log_options:
        enabled: false
        ttl_seconds: 604800
        async_writers: 1
        threshold_ms: 3000
    
    solr_update_handler_metrics_options:
        enabled: false
        ttl_seconds: 604800
        refresh_rate_ms: 60000
    
    solr_request_handler_metrics_options:
        enabled: false
        ttl_seconds: 604800
        refresh_rate_ms: 60000
    
    solr_index_stats_options:
        enabled: false
        ttl_seconds: 604800
        refresh_rate_ms: 60000
    
    solr_cache_stats_options:
        enabled: false
        ttl_seconds: 604800
        refresh_rate_ms: 60000
    
    solr_latency_snapshot_options:
        enabled: false
        ttl_seconds: 604800
        refresh_rate_ms: 60000
    
    # Node health is a score-based representation of how fit a node is to handle queries. The score is a
    # function of how long a node has been up and the rate of dropped mutations in the recent past.
    node_health_options:
        refresh_rate_ms: 60000
        # The amount of continuous uptime required for the node to reach the maximum uptime score. If you
        # are concerned with consistency during repair after a period of downtime, you may want to
        # temporarily increase this time to the expected time it will take to complete repair.
        #
        # Default - 10800 seconds (3 hours)
        uptime_ramp_up_period_seconds: 10800
        # The time window in the past over which the rate of dropped mutations affects the node health score.
        # Default - 30 minutes
        dropped_mutation_window_minutes: 30
    
    # If enabled (true), replica selection for distributed DSE Search queries takes node health into account
    # when multiple candidates exist for a particular token range. Set to false to ignore
    # node health when choosing replicas.
    #
    # Health-based routing allows us to make a trade-off between index consistency and query throughput. If
    # the primary concern is query performance, it may make sense to set this to "false".
    #
    # Default is enabled (true).
    enable_health_based_routing: true
    
    # If enabled (true), DSE Search reindexing of bootstrapped data will happen asynchronously, and the node will join the ring straight
    # after bootstrap.
    #
    # Default is disabled (false). The node will wait for reindexing of bootstrapped data to finish before joining the ring.
    async_bootstrap_reindex: false
    
    # Lease metrics. Enable these metrics to help monitor the performance of the lease subsystem.
    # ttl_seconds controls how long the log of lease holder changes persists.
    lease_metrics_options:
        enabled: false
        ttl_seconds: 604800
    
    # The directory where system keys are kept.
    #
    # Keys used for SSTable encryption must be distributed to all nodes.
    # DSE must be able to read and write to the directory.
    #
    # This directory should have 700 permissions and belong to the dse user.
    system_key_directory: /etc/dse/conf
    
    # If this is set to true, DSE requires the following config values to be encrypted:
    #     resources/cassandra/conf/cassandra.yaml:
    #         server_encryption_options.keystore_password
    #         server_encryption_options.truststore_password
    #         client_encryption_options.keystore_password
    #         client_encryption_options.truststore_password
    #    resources/dse/conf/dse.yaml:
    #         ldap_options.search_password
    #         ldap_options.truststore_password
    #
    # It's an error if the passwords aren't encrypted.
    # Config values can be encrypted with "dsetool encryptconfigvalue"
    config_encryption_active: false
    
    # The name of the system key used to encrypt / decrypt passwords stored
    # in configuration files.
    #
    # If config_encryption_active is true, it's an error if a valid key with
    # this name isn't in the system key directory keyfiles, and KMIP managed
    # keys can be created with "dsetool createsystemkey"
    config_encryption_key_name: system_key
    
    ##########################
    # Spark-related settings
    
    # The length of a shared secret used to authenticate Spark components and encrypt the connections between them.
    # Note that this is not the strength of the cipher used for encrypting connections.
    spark_shared_secret_bit_length: 256
    
    # Enables Spark security based on shared secret infrastructure. Enables mutual authentication between Spark master
    # and worker nodes.
    spark_security_enabled: false
    
    # Enables encryption between Spark master and worker nodes, except Web UI. The connection uses the
    # Digest-MD5 SASL-based encryption mechanism. This option applies only if spark_security_enabled is true.
    spark_security_encryption_enabled: false
    
    # # How often Spark plugin should check for Spark Master / Spark Worker readiness to start. The value is
    # # a time (in ms) between subsequent retries.
    # spark_daemon_readiness_assertion_interval: 1000
    
    # Controls the physical resources that can be used by Spark applications on this node.
    # cores_total is the number of cores and and memory_total is total system memory that you can assign to all executors
    # that are run by the work pools on this node. The values can be absolute (exact number of cores) or the
    # memory size (use metric suffixes like M for mega, and G for giga) or a fraction of physical cores reported by the OS,
    # and fraction of available memory, where available memory is calculated as: total physical memory - DSE max heap size.
    # cores_total and memory_total replace initial_spark_worker_resources option which was used in earlier DSE versions.
    # The default 0.7 for cores and memory corresponds to the default value of initial_spark_worker_resources 0.7.
    # DSE does not support setting Spark Worker cores and memory through environment variables SPARK_WORKER_CORES
    # and SPARK_WORKER_MEMORY.
    # resource_manager_options:
    #     worker_options:
    #         cores_total: 0.7
    #         memory_total: 0.6
    #
    #         workpools:
    #             - name: alwayson_sql
    #               cores: 0.25
    #               memory: 0.25
    
    # In DSE 5.1 and later: Communication between Spark applications and the resource manager are routed through
    # the CQL native protocol. Enabling client encryption in cassandra.yaml will also enable encryption for
    # the communication with the DSE Spark Master. To secure the communication between Spark Driver and Spark Executors,
    # enable Spark authentication and encryption for that application.
    # In contrast, mutual authentication and encryption of communication between DSE Spark Master and Workers are
    # managed by spark_security_enabled and spark_security_encryption_enabled in dse.yaml.
    
    # Spark UI options apply to Spark Master and Spark Worker UIs and to Spark daemon UIs in general. Spark UI options do NOT
    # apply to user applications even if they run in cluster mode.
    spark_ui_options:
        # Valid values are:
        # inherit - SSL settings are inherited from DSE client encryption options
        # custom - SSL settings from encryption_options below
        encryption: inherit
    
        encryption_options:
            enabled: false
            keystore: resources/dse/conf/.ui-keystore
            keystore_password: cassandra
            # require_client_auth: false
            # Set trustore and truststore_password if require_client_auth is true
            # truststore: resources/dse/conf/.ui-truststore
            # truststore_password: cassandra
            # More advanced defaults:
            # protocol: TLS
            # algorithm: SunX509
            # store_type: JKS
            # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]
    
    # Configure how the driver and executor processes are created and managed.
    spark_process_runner:
        # Valid options are: default, run_as
        runner_type: default
    
        # DSE uses sudo to run Spark application components (drivers and executors) as specific OS users.
        # A set of predefined users, called slot users, is used for this purpose. All drivers and executors
        # owned by some DSE user are run as some slot user x. Drivers and executors of any other DSE user
        # use different slots.
        # Setting up slots:
        # 1. Create n users (n = number of slots), call them slot1, slot2, ..., slotn, with no login. Each user
        #    should have primary group the same as its name, so for example slot1:slot1, slot2:slot2, ...
        # 2. Add DSE service user (the user who runs DSE server) to the slot user groups; the DSE service user must be
        #    in all slot user groups.
        # 3. Modify the sudoers files so that:
        #    a) DSE service user can execute any command as any slot user without providing a password
        #    b) umask is overridden to 007 for those commands so that files created by sub-processes will not be accessible
        #       by anyone by default,
        #    For example, if we have two slot users slot1, slot2, and DSE service user dse, add these slot users to sudoers:
        #    Runas_Alias     SLOTS = slot1, slot2
        #    Defaults>SLOTS  umask=007
        #    Defaults>SLOTS  umask_override
        #    dse             ALL=(SLOTS) NOPASSWD: ALL
        run_as_runner_options:
            user_slots:
                - slot1
                - slot2
    
    # AlwaysOn SQL options have dependence on workpool setting of resource_manager_options. Set workpool configuration if you
    # enable alwayson_sql_options.
    # alwayson_sql_options:
    #     # Set to true to enable the node for AlwaysOn SQL. Only an Analytics node
    #     # can be enabled as an AlwaysOn SQL node.
    #     enabled: false
    #
    #     # AlwaysOn SQL Thrift port
    #     thrift_port: 10000
    #
    #     # AlwaysOn SQL WebUI port
    #     web_ui_port: 9077
    #
    #     # The waiting time to reserve the Thrift port if it's not available
    #     reserve_port_wait_time_ms: 100
    #
    #     # The waiting time to check AlwaysOn SQL health status
    #     alwayson_sql_status_check_wait_time_ms: 500
    #
    #     # The work pool name used by AlwaysOn SQL
    #     workpool: alwayson_sql
    #
    #     # Location in DSEFS of the log files
    #     log_dsefs_dir: /spark/log/alwayson_sql
    #
    #     # The role to use for internal communication by AlwaysOn SQL if authentication is enabled
    #     auth_user: alwayson_sql
    #
    #     # The maximum number of errors that can occur during AlwaysOn SQL service runner thread
    #     # runs before stopping the service. A service stop requires a manual restart.
    #     runner_max_errors: 10
    
    ##########################
    # DSE File System (DSEFS) options
    # dsefs_options:
    #
    #     # Whether to enable DSEFS on this node.
    #     # If not set, DSEFS is enabled only on the nodes that run a Spark workload.
    #     enabled:
    #
    #     # The keyspace where the DSEFS metadata is stored. Optionally configure multiple DSEFS file systems
    #     # within a cluster by specifying a different keyspace name for each datacenter.
    #     keyspace_name: dsefs
    #
    #     # The local directory for storing the local node metadata, including the node identifier.
    #     # The amount of data stored is nominal, and does not require configuration for throughput, latency, or capacity.
    #     # This directory must not be shared by DSEFS nodes.
    #     work_dir: /var/lib/dsefs
    #
    #     # The public port on which DSEFS listens for clients. The service on this port is bound to
    #     # native_transport address.
    #     public_port: 5598
    #
    #     # Port for inter-node communication, must be not visible from outside of the cluster.
    #     # It is bound to listen address. Do not open this port to firewalls.
    #     private_port: 5599
    #
    #     # Mandatory attribute to identify the set of directories. DataStax recommends segregating these data directories
    #     # on physical devices that are different from the devices that are used for the DSE database.
    #     # Using multiple directories on JBOD improves performance and capacity.
    #     data_directories:
    #         - dir: /var/lib/dsefs/data
    #
    #           # The weighting factor for this location specifies how much data to place in this directory, relative to
    #           # other directories in the cluster. This soft constraint determines how DSEFS distributes the data.
    #           storage_weight: 1.0
    #
    #           # Reserved space (in bytes) that is not going to be used for storing blocks
    #           min_free_space: 5368709120
    #
    #     # More advanced settings:
    #
    #     # Wait time before the DSEFS server times out while waiting for services to bootstrap.
    #     service_startup_timeout_ms: 600000
    #
    #     # Wait time before the DSEFS server times out while waiting for services to close.
    #     service_close_timeout_ms: 600000
    #
    #     # Wait time that the DSEFS server waits during shutdown before closing all pending connections.
    #     server_close_timeout_ms: 2147483647 # Integer.MAX_VALUE
    #
    #     # The maximum accepted size of a compression frame defined during file upload.
    #     compression_frame_max_size: 1048576
    #
    #     # Maximum number of elements in a single DSEFS Server query cache. DSEFS reuses this value for every cache that
    #     # stores database query results.
    #     query_cache_size: 2048
    #
    #     # The time to retain the DSEFS Server query cache element in cache. The cache element expires
    #     # when this time is exceeded.
    #     query_cache_expire_after_ms: 2000
    #
    #     gossip_options:
    #         # The delay between gossip rounds
    #         round_delay_ms: 2000
    #
    #         # How long to wait after registering the Location and reading back all other Locations from the database
    #         startup_delay_ms: 5000
    #
    #         # How long to wait after announcing shutdown before shutting down the node
    #         shutdown_delay_ms: 10000
    #
    #     rest_options:
    #         # How long RestClient is going to wait for a response corresponding to a given request
    #         request_timeout_ms: 330000
    #
    #         # How long RestClient is going to wait for establishing a new connection
    #         connection_open_timeout_ms: 10000
    #
    #         # How long RestClient is going to wait until all pending transfers are complete before closing
    #         client_close_timeout_ms: 60000
    #
    #         # How long to wait for the server rest call to complete
    #         server_request_timeout_ms: 300000
    #
    #         # How long RestClient is going to wait until idle connection is closed, 0 if disabled.
    #         # If RestClient does not close connection after this timeout, the server closes the connection after
    #         # 2 * idle_connection_timeout_ms milliseconds.
    #         idle_connection_timeout_ms: 60000
    #
    #         # How long to wait until idle internode connection is closed, 0 if disabled.
    #         # The internode connections are mainly used to exchange data during replication. Heavily utilized DSEFS
    #         # clusters should not make this value lower.
    #         internode_idle_connection_timeout_ms: 120000
    #
    #         # Maximum number of connections to a given host per single CPU core. DSEFS keeps a connection pool for
    #         # each CPU core.
    #         core_max_concurrent_connections_per_host: 8
    #
    #     transaction_options:
    #         # How long to allow a transaction to run before considering it for timing out and rollback
    #         transaction_timeout_ms: 60000
    #
    #         # How long to wait before retrying a transaction aborted due to a conflict
    #         conflict_retry_delay_ms: 10
    #
    #         # How many times the transaction is retried in case of a conflict before giving up
    #         conflict_retry_count: 40
    #
    #         # How long to wait before retrying a failed transaction payload execution
    #         execution_retry_delay_ms: 1000
    #
    #         # How many times to retry executing the payload before signaling the error to the application
    #         execution_retry_count: 3
    #
    #     block_allocator_options:
    #         # The overflow_margin_mb and overflow_factor options control how much additional data can be placed
    #         # on the local (coordinator) before the local node overflows to the other nodes.
    #         # A local node is preferred for a new block allocation, if
    #         # used_size_on_the_local_node < average_used_size_per_node * overflow_factor + overflow_margin.
    #         # The trade-off is between data locality of writes and balancing the cluster.
    #         # To disable the preference for allocating blocks on the coordinator node, set these values to 0 MB and 1.0.
    #         overflow_margin_mb: 1024
    #         overflow_factor: 1.05
    
    ##########################
    # Audit logging options
    audit_logging_options:
        enabled: false
    
        # The logger used for logging audit information
        # Available loggers are:
        #   CassandraAuditWriter - logs audit info to a cassandra table. This logger can be run synchronously or
        #                          asynchronously. Audit logs are stored in the dse_audit.audit_log table.
        #                          When run synchronously, a query will not execute until it has been written
        #                          to the audit log table successfully. If a failure occurs before an audit event is
        #                          written, and it's query is executed, the audit logs might contain queries that were never
        #                          executed.
        #   SLF4JAuditWriter -     logs audit info to an SLF4J logger. The logger name is `SLF4JAuditWriter`,
        #                          and can be configured in the logback.xml file.
        logger: SLF4JAuditWriter
    
    #     # Comma-separated list of audit event categories to be included or excluded from the audit log.
    #     # When not set, the default includes all categories.
    #     # Categories are: QUERY, DML, DDL, DCL, AUTH, ADMIN, ERROR.
    #     # Specify either included or excluded categories. Specifying both is an error.
    #     included_categories:
    #     excluded_categories:
    
    #     # Comma-separated list of keyspaces to be included or excluded from the audit log.
    #     # When not set, the default includes all keyspaces.
    #     # Specify either included or excluded keyspaces. Specifying both is an error.
    #     included_keyspaces:
    #     excluded_keyspaces:
    
    #     # Comma separated list of the roles to be audited or not.
    #     # Specify either included or excluded roles. Specifying both is an error
    #     included_roles:
    #     excluded_roles:
    
        # The amount of time, in hours, audit events are retained by supporting loggers.
        # Only the CassandraAuditWriter supports retention time.
        # Values of 0 or less retain events forever.
        retention_time: 0
    
        cassandra_audit_writer_options:
            # Sets the mode the audit writer runs in.
            #
            # When run synchronously, a query is not executed until the audit event is successfully written.
            #
            # When run asynchronously, audit events are queued for writing to the audit table, but are
            # not necessarily logged before the query executes. A pool of writer threads consumes the
            # audit events from the queue, and writes them to the audit table in batch queries. While
            # this substantially improves performance under load, if there is a failure between when
            # a query is executed, and it's audit event is written to the table, the audit table may
            # be missing entries for queries that were executed.
            # valid options are 'sync' and 'async'
            mode: sync
    
            # The maximum number of events the writer will dequeue before writing them out to the table.
            # If you're seeing warnings in your logs about batches being too large, decrease this value.
            # Increasing batch_size_warn_threshold_in_kb in cassandra.yaml is also an option, but make sure you understand
            # the implications before doing so.
            #
            # Only used in async mode. Must be >0
            batch_size: 50
    
            # The maximum amount of time in milliseconds an event will be dequeued by a writer before being written out. This
            # prevents events from waiting too long before being written to the table when there's not a lot of queries happening.
            #
            # Only used in async mode. Must be >0
            flush_time: 250
    
            # The size of the queue feeding the asynchronous audit log writer threads. When there are more events being
            # produced than the writers can write out, the queue will fill up, and newer queries will block until there
            # is space on the queue.
            # If a value of 0 is used, the queue size will be unbounded, which can lead to resource exhaustion under
            # heavy query load.
            queue_size: 30000
    
            # the consistency level used to write audit events
            write_consistency: QUORUM
    
    #         # Where dropped events are logged
    #         dropped_event_log: /var/log/cassandra/dropped_audit_events.log
    
    #         # Partition days into hours by default
    #         day_partition_millis: 3600000
    
    ##########################
    # System information encryption settings
    #
    # If enabled, system tables that might contain sensitive information (system.batchlog,
    # system.paxos), hints files, and Cassandra commit logs are encrypted with these
    # encryption settings.
    #
    # If DSE Search index encryption is enabled, DSE Search index files are also encrypted with these settings.
    # If backing C* table encryption is enabled, DSE Search commit log is encrypted with these settings.
    #
    # When enabling system table encryption on a node with existing data, run
    # `nodetool upgradesstables -a` on the listed tables to encrypt existing data.
    #
    # When tracing is enabled, sensitive information is written to the tables in the
    # system_traces keyspace. Configure encryption on the tables to encrypt their data
    # on disk by using an encrypting compressor.
    #
    # DataStax recommends using remote encryption keys from a KMIP server when using Transparent Data Encryption (TDE) features.
    # Local key support is provided when a KMIP server is not available.
    system_info_encryption:
        enabled: false
        cipher_algorithm: AES
        secret_key_strength: 128
        chunk_length_kb: 64
    
    #     # The encryptor will use a KMIP key server to manage its encryption keys. Specify only to use a KMIP key server,
    #     # otherwise omit this entry. The default is to use local key encryption.
    #     key_provider: KmipKeyProviderFactory
    
    #     # If KmipKeyProviderFactory is used for system_info_encryption, this specifies the kmip host to be used.
    #     kmip_host: kmip_host_name
    
    ##########################
    # KMIP hosts options
    #
    # Connection settings for key servers supporting the KMIP protocol
    # allow DSE encryption features to use encryption and decryption keys that are not stored
    # on the same machine running DSE.
    #
    # Hosts are configured as <kmip_host_name>: {connection_settings}, which maps a user-defined
    # name to a set of KMIP hosts and KMIP-defined credentials (keystores and truststores) that are used with a particular
    # key server. This name is then used when referring to KMIP hosts. DSE supports multiple KMIP hosts.
    
    # kmip_hosts:
    #     # The unique name of this KMIP host/cluster which is specified in the table schema.
    #     host.yourdomain.com:
    #
    #         # Comma-separated list of KMIP hosts host[:port]
    #         # The current implementation of KMIP connection management supports only failover, so all requests will
    #         # go through a single KMIP server. There is no load balancing. This is because there aren't many known KMIP servers
    #         # that support read replication, or other strategies for availability.
    #         #
    #         # Hosts are tried in the order they appear, so add KMIP hosts in the intended failover sequence.
    #         hosts: kmip1.yourdomain.com, kmip2.yourdomain.com
    #
    #         # keystore/truststore info
    #         keystore_path: /path/to/keystore.jks
    #         keystore_type: jks
    #         keystore_password: password
    #
    #         truststore_path: /path/to/truststore.jks,
    #         truststore_type: jks
    #         truststore_password: password
    #
    #         # The time that keys read from the KMIP hosts are cached locally.
    #         # The longer keys are cached, the fewer requests are made to the key server. However, also sets the time
    #         # for changes (ie: revocation) to propagate to the DSE node.
    #         key_cache_millis: 300000
    #
    #         # Socket timeout in milliseconds.
    #         timeout: 1000
    
    # # driver - DSE Search will use Solr cursor paging (deep paging) when pagination is enabled by the CQL driver.
    # #
    # # off - DSE Search will ignore the driver's pagination settings and use normal Solr paging unless:
    # #   - The current workload is an analytics workload (ex. SearchAnalytics).
    # #   - The query parameter 'paging' is set to 'driver'.
    # #
    # # Default is 'off'
    # #
    # cql_solr_query_paging: off
    
    # Local settings for tiered storage
    #
    # Tiered storage supports multiple disk configurations that are configured as <config_name> : <config_settings>, and specified in DDL.
    # The tiers themselves are unnamed, and are just collections of paths that must be defined in the order they're to be used.
    # Typically, put your fastest storage in the top tier, and go down from there.
    #
    # Storage configurations don't need to be homogenous across the cluster, and internally, each node will use only the
    # the number of tiers it has configured, or the number of tiers configured to be used in the DDL, whichever is less.
    #
    # Although the behavior of the tiered storage strategy for a given table is configured in the DDL, these settings can
    # be overridden locally, per node, by specifying 'local_options' : {<k>:<v>, ...} in a table schema. This can be useful for testing
    # options before deploying cluster wide, or for storage configurations which don't map cleanly to the DDL configuration.
    #
    # tiered_storage_options:
    #     strategy1:
    #         tiers:
    #             - paths:
    #                 - /mnt1
    #                 - /mnt2
    #             - paths: [ /mnt3, /mnt4 ]
    #             - paths: [ /mnt5, /mnt6 ]
    #
    #         local_options:
    #             k1: v1
    #             k2: v2
    #
    #     'another strategy':
    #         tiers: [ paths: [ /mnt1 ] ]
    
    ##########################
    # DSE Advanced Replication configuration settings
    #
    # DSE Advanced replication supports one-way distributed data replication from remote
    # clusters (source clusters) to central data hubs (destination clusters).
    #
    # advanced_replication_options:
    #     enabled: false
    #     # Whether to enable driver password encryption. Driver passwords are stored in a CQL table.
    #     # DataStax recommends encrypting the driver passwords before you add them to the CQL table.
    #     # By default, driver user names and passwords are plain text. When true, the configured passwords
    #     # (including Cassandra password, SSL keystore/truststore password, etc.) that are stored in the
    #     # advrep config must be encrypted and generated as system keys. Each node in the source cluster must have the same
    #     # encryption/decryption key. The destination cluster does not require this key.
    
    #     conf_driver_password_encryption_enabled: false
    
    #     # The directory to hold advanced replication log files.
    #     advanced_replication_directory: /var/lib/cassandra/advrep
    
    #     # The base path that will be prepended to paths in the Advanced Replication
    #     # configuration locations, including locations to SSL keystore, SSL truststore, etc.
    #     security_base_path: /base/path/to/advrep/security/files/
    
    ##########################
    # These internode_messaging_options configure network services for internal communication
    # for all nodes. These settings must be identical on all nodes in the cluster.
    internode_messaging_options:
        # TCP listen port (mandatory)
        port: 8609
    
    #     # Maximum message frame length. If not set, the default is 256 MB.
    #     frame_length_in_mb: 256
    
    #     # Number of server acceptor threads. If not set, the default is the number of available processors.
    #     server_acceptor_threads: 8
    
    #     # Number of server worker threads. If not set, the default is the number of available processors * 8.
    #     server_worker_threads: 16
    
    #     # Maximum number of client connections. If not set, the default is 100.
    #     client_max_connections: 100
    
    #     # Number of client worker threads. If not set, the default is the number of available processors * 8.
    #     client_worker_threads: 16
    
    #     # Timeout for communication handshake process. If not set, the default is 10 seconds.
    #     handshake_timeout_seconds: 10
    
    #     # Client request timeout. If not set, the default is 60 seconds.
    #     client_request_timeout_seconds: 60
    
    ##########################
    # Graph configuration
    # Contains all system-level configuration options and those shared between graph
    # instances.
    graph:
        # Maximum time to wait for an OLAP analytic (Spark) traversal to evaluate.
        # When not set, the default is 10080 minutes (168 hours).
        # analytic_evaluation_timeout_in_minutes: 10080
    
        # Maximum time to wait for an OLTP real-time traversal to evaluate.
        # When not set, the default is 10080 minutes (168 hours).
        # realtime_evaluation_timeout_in_seconds: 30
    
        # Maximum time to wait for the database to agree on schema versions before timing
        # out. When not set, the default is 10000 ms (10 seconds).
        # schema_agreement_timeout_in_ms: 10000
    
        # Maximum time to wait for a graph-system request to evaluate. Creating a new
        # graph is an example of a graph-system request.
        # When not set, the default is 180 seconds.
        # system_evaluation_timeout_in_seconds: 180
    
        # The amount of memory (RAM) to allocate to each graph's adjacency (edge and property)
        # cache. When not set, the default is 128. Value: integer.
        # adjacency_cache_size_in_mb: 128
    
        # The amount of memory (RAM) to allocate to the index cache. Value: integer.
        # When not set, the default is 128. Value: integer.
        # index_cache_size_in_mb: 128
    
        # The maximum number of CQL queries that can be queued as a result of Gremlin
        # requests. Incoming queries are rejected if the queue size exceeds this setting.
        # When not set, the default is 10000. Value: integer.
        # max_query_queue: 10000
    
        # The maximum number of threads to use for graph queries on the database.
        # When not not set, the effective default is 10 times either the gremlinPool
        # setting (if gremlinPool is present in this file and nonzero), or the number of
        # available CPU cores (if gremlinPool is not present in this file or set to zero).
        # The gremlinPool setting is in the gremlin_server subsection of the graph
        # section in dse.yaml. Value: integer.
        # max_query_threads (no explicit default)
    
        # The maximum number of parameters that can be passed on a graph query request for both TinkerPop drivers
        # and those using the Cassandra native protocol. Generally speaking, it is considered an anti-pattern to
        # pass "massive" numbers of parameters on requests, as it increases the script evaluation time. Consider
        # other methods for parameterizing scripts (like passing a single Map or List if many arguments are needed)
        # before you increase this value.
        # max_query_params: 16
    
        gremlin_server:
            # port: 8182
    
            # Size of the worker thread pool. Should generally not exceed 2 * number of cores.
            # A worker thread performs non-blocking read and write for one or more Channels.
            # threadPoolWorker: 2
    
            # The number of "Gremlin" threads available to execute actual scripts in a ScriptEngine. This pool represents
            # the workers available to handle blocking operations in Gremlin Server. When set to zero, this value will
            # be defaulted to the value of the JVM property "cassandra.available_processors" (if set)
            # or to Runtime.getRuntime().availableProcessors().
            # gremlinPool: 0
    
    #        # The gremlin-groovy script engine will always be added even if the configuration option is not present.
    #        # Additional imports may be added in the configuration for that script engine.
    #         scriptEngines:
    #            gremlin-groovy:
    #                 config:
    #                     # To disable the gremlin groovy sandbox entirely
    #                     sandbox_enabled: false
    #                     sandbox_rules:
    #
    #                         # To completely whitelist a package add the package name here
    #                         whitelist_packages:
    #                         - package.name
    #
    #                         # To whitelist an individual type add the name of the type here
    #                         whitelist_types:
    #                         - fully.qualified.class.name
    #
    #                         # To whitelist a super class add the name of the type here
    #                         whitelist_supers:
    #                         - fully.qualified.class.name
    
    
    
  

---
# Source: dse-server/templates/storage.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd

---
# Source: dse-server/templates/dse.yaml
apiVersion: v1
kind: Service
metadata:
  name: dse-dse-server
  namespace: "dev"
  labels: &Labels
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - port: 9042
    name: cql-port
  selector:
    app: dse-dse-server
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: dse-dse-server
  namespace: "dev"
  labels: &Labels
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
spec:
  serviceName: dse-dse-server
  replicas: 3
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels: *Labels
    spec:
      securityContext:
        fsGroup: 999
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: 
                - dse-dse-server
            topologyKey: kubernetes.io/hostname
      initContainers:
      - name: init-opsc-up
        image: "gcr.io/datastax-public/dse-curl:6.1"
        command: ['sh', '-c', 'until curl -k https://${OPSC_DNS}:8443; do echo waiting for OPSC; sleep 5; done;']
        env:
        - name: OPSC_DNS
          value: "dse-dse-server-opsc-0.dse-dse-server-opsc.dev.svc.cluster.local"
      containers:
      - name: dse-dse-server
        image: "gcr.io/datastax-public/dse:6.1"
        imagePullPolicy: "IfNotPresent"
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
        env:
        - name: DS_LICENSE
          value: accept
        - name: SEEDS
          value: "dse-dse-server-0.dse-dse-server.dev.svc.cluster.local,"
        - name: CLUSTER_NAME
          value: "My_Cluster"
        - name: NUM_TOKENS
          value: "256"
        - name: DC
          value: "DC1"
        - name: RACK
          value: "RAC1"
        - name: SNITCH
          value: "GossipingPropertyFileSnitch"
        - name: OPSCENTER_IP
          value: dse-dse-server-opsc-0.dse-dse-server-opsc.dev.svc.cluster.local
        ports:
        - containerPort: 7000
          name: intra-node-port
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx-port
        - containerPort: 8609
          name: inter-node-msg
        - containerPort: 9042
          name: cql-port
        - containerPort: 9160
          name: thrift-port
        - containerPort: 8983
          name: solr-port
        - containerPort: 8984
          name: solr-int-node
        - containerPort: 4040
          name: spark-app-port
        - containerPort: 7077
          name: spark-int-node
        - containerPort: 7080
          name: spark-m-web
        - containerPort: 7081
          name: spark-w-web
        - containerPort: 8090
          name: spark-job
        - containerPort: 9999
          name: spark-job-jmx
        - containerPort: 18080
          name: spark-histor
        - containerPort: 8182
          name: gremlin-port
        - containerPort: 5598
          name: dsefs-public
        - containerPort: 5599
          name: dsefs-private
        - containerPort: 61621
          name: ds-agent-port
        volumeMounts:
        - name: dse-data
          mountPath: /var/lib/cassandra
        livenessProbe:
          tcpSocket:
            port: 9042
          initialDelaySeconds: 900
          timeoutSeconds: 1
          periodSeconds: 30
          failureThreshold: 10
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "exec nodetool decommission"]
  volumeClaimTemplates:
  - metadata:
      name: dse-data
      labels: *Labels
      annotations:
        volume.beta.kubernetes.io/storage-class: fast
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: "60Gi"

---
# Source: dse-server/templates/opscenter.yaml
apiVersion: v1
kind: Service
metadata:
  name: dse-dse-server-opsc-ext-lb
  namespace: "dev"
  labels: &Labels
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server-opsc
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
spec:
  type: LoadBalancer
  ports:
  - port: 8443
    name: opsc-secure-port
  selector:
    app: dse-dse-server-opsc
---
apiVersion: v1
kind: Service
metadata:
  name: dse-dse-server-opsc
  namespace: "dev"
  labels: &Labels
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server-opsc
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
spec:
  clusterIP: None
  ports:
  - port: 8888
    name: opsc-gui-port
  - port: 8443
    name: opsc-secure-port
  - port: 61620
    name: port-61620
  selector:
    app: dse-dse-server-opsc
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: dse-dse-server-opsc
  namespace: "dev"
  labels: &Labels
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server-opsc
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
spec:
  serviceName: "dse-dse-server-opsc"
  replicas: 1
  template:
    metadata:
      labels: *Labels
    spec:
      containers:
        - name: dse-dse-server-opsc
          image: "gcr.io/datastax-public/dse/dse-opscenter-650:6.1"  
          imagePullPolicy: "IfNotPresent"
          lifecycle:
            postStart:
              exec:
                command: ["/update_admin_password.sh"]
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
          env:
          - name: DS_LICENSE
            value: accept
          - name: OPSC_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: dse-dse-server-opsc-admin-pwd-secret
                key: password
          ports:
          - containerPort: 8888
            name: opsc-gui-port
          - containerPort: 61620
            name: port-61620
          volumeMounts:
          - name: config-clusters-volume
            mountPath: /opt/opscenter/conf/clusters
          - name: opsc-conf-volume
            mountPath: /config
          - name: opsc-ssl-volume
            mountPath: /var/lib/opscenter/ssl
      volumes:
      - name: opsc-conf-volume
        configMap:
          name: dse-dse-server-opsc-conf-configmap
      - name: opsc-ssl-volume
        configMap:
          name: dse-dse-server-opsc-ssl-configmap
  volumeClaimTemplates:
  - metadata:
      name: config-clusters-volume
      annotations:
        volume.beta.kubernetes.io/storage-class: fast
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 1Gi

---
# Source: dse-server/templates/dse_cluster_init.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: dse-dse-server-dse-cluster-init-job
  namespace: "dev"
  labels: &Labels
    app.kubernetes.io/name: "dse"
    app.kubernetes.io/component: dse
    app: dse-dse-server
    chart: "dse-server-0.1.0"
    release: "dse"
    heritage: "Tiller"
    component: dse
spec:
  template:
    spec:
      initContainers:
      - name: init-opsc-up
        image: "gcr.io/datastax-public/dse-curl:6.1"
        command: ['sh', '-c', 'until curl -k https://${OPSC_DNS}:8443; do echo waiting for OPSC; sleep 5; done;']
        env:
        - name: OPSC_DNS
          value: "dse-dse-server-opsc-0.dse-dse-server-opsc.dev.svc.cluster.local"
      containers:
      - name:  dse-dse-server-dse-cluster-init
        image: "gcr.io/datastax-public/dse/dse-cluster-init:6.1" 
        imagePullPolicy: "IfNotPresent"
        env:
        - name: SEED_DNS
          value: "dse-dse-server-0.dse-dse-server.dev.svc.cluster.local"
        - name: OPSC_DNS
          value: "dse-dse-server-opsc-0.dse-dse-server-opsc.dev.svc.cluster.local"
        - name: CLUSTER_NAME
          value: "My_Cluster"
        - name: CLUSTER_SIZE
          value: "3"
        - name: OPSC_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: dse-dse-server-opsc-admin-pwd-secret
              key: password
      restartPolicy: Never
  backoffLimit: 8

---
# Source: dse-server/templates/application.yaml
---
apiVersion: app.k8s.io/v1beta1
kind: Application
metadata:
  name: "dse"
  namespace: "dev"
  annotations:
    kubernetes-engine.cloud.google.com/icon: >-
      data:image/png;base64,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
    kubernetes-engine.cloud.google.com/support: >-
      The use of DataStax software in Google Launcher solution is intended for development purpose only. It includes a limited no-fee license from DataStax. As described in section 1.4 of DataStax Enterprise Terms, the limited license is for development or non-production use. Users can email partner-architecture@datastax.com for support of issues related to this deployment.
    marketplace.cloud.google.com/deploy-info: '{partner_id: "datastax-public", product_id: "datastax-enterprise-gke", partner_name: "Datastax"}'
  labels:
    app.kubernetes.io/name: "dse"
spec:
  descriptor:
    type: dse
    versions: '0.1'
    description: |-
      DataStax Enterprise (DSE) is the always-on, Active Everywhere distributed hybrid cloud database
      built on Apache CassandraTM. The foundation for personalized, real-time applications at massive
      scale, DSE makes it easy for enterprises to exploit hybrid and multi-cloud environments via a
      seamless data layer that eliminates the issues that typically come with deploying applications
      across multiple on-premises data centers and/or multiple public clouds. DSE also gives businesses
      full data visibility, portability, and control, allowing them to retain strategic ownership of
      their most valuable asset in a hybrid/multi-cloud world.

      There are minimum cluster requirements that MUST be met for the deployment to succeed. Please 
      ensure you have a cluster meeting these minimums before deploying. The requirements are 5 nodes 
      of instance type n1-standard-4 with at least 60GB of disk size. 
    notes: |-
      # Minimum requirements to launch DSE solution

      Your GKE cluster needs to have minimum of 5 nodes with machine type n1-standard-4 or higher. 

      # Access DataStax Enterprise (DSE) OpsCenter web console

      It will take about roughly 10 minutes to complete the installation.

      Get the external IP of the DSE OpsCenter and visit
      the URL printed below in your browser.

      ```
      SERVICE_IP=$(kubectl get \
        --namespace  \
        svc -dse-server-opsc-ext-lb \
        -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

      echo "https://${SERVICE_IP}:8443"
      ```
      
      Then log in using username, "admin" and password produced
      by running the following command.

      ```
      echo -n `(kubectl get \
      --namespace   \
      secret -opsc-admin-pwd-secret \
      -o jsonpath='{.data.password}')` \
      | base64 -D 
      ```
    maintainers:
    - name: Google Click to Deploy
      url: https://cloud.google.com/solutions/#click-to-deploy
    links:
    - description: Getting Started
      url: https://github.com/DSPN/google-deployment-guide/blob/master/gkelauncher.md 
      description: The easiest way to learn Apache Cassandra™ and DataStax Enterprise at DataStax Academy
      url: https://academy.datastax.com/courses
  selector:
    matchLabels:
      app.kubernetes.io/name: "dse"
  componentKinds:
  - group: apps/v1beta1 
    kind: StatefulSet
  - group: v1
    kind: PersistentVolumeClaim
  - group: v1
    kind: Service
  - group: v1
    kind: Secret
  - group: batch/v1
    kind: Job
  - group: rbac.authorization.k8s.io/v1
    kind: Role
  - group: rbac.authorization.k8s.io/v1
    kind: RoleBinding
  - group: v1
    kind: ServiceAccount
  - group: v1
    kind: ConfigMap

